## 关键词
1. 召回
2. 搜索相关性
3. query改写/理解
4. bert/ernie
5. paddle/pytorch
6. 文本相关性
7. 向量化/检索
8. 预训练/推理资源

## 前沿技术
1. EMNLP 2023 Best Paper公布啦：https://mp.weixin.qq.com/s/NQ2eLJ47Oni85O7m1oGWtA
2. sigir‘23「快手」SESRec: When Search Meets Recommendation: Learning Disentangled Search Representation - 猫的薛定谔的文章 - 知乎 https://zhuanlan.zhihu.com/p/670956973
3. [VLDB 2019] 如何为 HTAP 负载寻找最优的列式存储布局 - 马克刘的文章 - 知乎 https://zhuanlan.zhihu.com/p/668222560
4. MoE 入门介绍 核心工作回顾 模型篇 - 原石人类的文章 - 知乎 https://zhuanlan.zhihu.com/p/671434414
5. CIKM'22 腾讯 | 召回模型大一统：U2I/U2U/I2I召回联合建模 - Gordon Lee的文章 - 知乎 https://zhuanlan.zhihu.com/p/671371188
6. 【EMNLP 2023】基于大语言模型的复杂任务认知推理算法CogTree - 阿里灵杰的文章 - 知乎 https://zhuanlan.zhihu.com/p/671744294
7. ACL 2023获奖论文全分享！NLP领域最新研究进展都在这了 - 鱼子酱的文章 - 知乎 https://zhuanlan.zhihu.com/p/643033773
8. 因果推断技术的实际应用落地情况是怎样的？ - 阿里妈妈技术的回答 - 知乎 https://www.zhihu.com/question/515386955/answer/2609255138
9. 《LLM+搜索召排》10篇论文一览 - 情迷搜广推的文章 - 知乎 https://zhuanlan.zhihu.com/p/672777138
10. ATorch：蚂蚁开源PyTorch分布式训练扩展库，助你将硬件算力压榨到极致 - AI Infra的文章 - 知乎 https://zhuanlan.zhihu.com/p/674090806
11. 苹果最新研究：将LLM放在闪存推理显著提升推理效率 - 岳廷的文章 - 知乎 https://zhuanlan.zhihu.com/p/673812879
12. 超长序列推荐：如何让推荐系统“读懂”你的“人生轨迹” - AI Box专栏的文章 - 知乎 https://zhuanlan.zhihu.com/p/668343442
13. VLDB 2022有哪些值得关注的论文？ - Hsword的回答 - 知乎 https://www.zhihu.com/question/549857210/answer/2687128931
14. 【性能工具】HPC/ML/OS/SW性能工具总结 - eyesighting的文章 - 知乎 https://zhuanlan.zhihu.com/p/673459718
15. 世界范围内有哪些研究流处理（stream processing）的高校和团队？ - 孙挺Sunt的回答 - 知乎 https://www.zhihu.com/question/516225132/answer/2346639834
16. 谷歌出品 | TIGER:生成式检索推荐系统 - Houye的文章 - 知乎 https://zhuanlan.zhihu.com/p/674703547
17. 图解高级RAG技术 - iyacontrol的文章 - 知乎 https://zhuanlan.zhihu.com/p/674755232
18. AI编译优化---访存密集算子优化 - sunshinelala的文章 - 知乎 https://zhuanlan.zhihu.com/p/674804131
19. 商品推荐系统场景，商品点击序列信息有多重要？ - magicwt的回答 - 知乎 https://www.zhihu.com/question/635887488/answer/3333662517
20. 关于特征工程里面的特征选择法？ - 郑昀昊的回答 - 知乎 https://www.zhihu.com/question/274263273/answer/2819168772
21. 淘宝主搜 | 大模型在长尾Query改写召回上的实践 - Gordon Lee的文章 - 知乎 https://zhuanlan.zhihu.com/p/675421157
22. CIKM23向量检索5篇论文一览 - 情迷搜广推的文章 - 知乎 https://zhuanlan.zhihu.com/p/675179867
23. 12家研究机构、160页、参考了650篇论文：基础模型推理最全综述 Part3 - 北方的郎的文章 - 知乎 https://zhuanlan.zhihu.com/p/675541255
24. 从 0 手撸一个 pytorch - 易迟的文章 - 知乎 https://zhuanlan.zhihu.com/p/675673150
25. A Survey on Large Language Models for Recommendation：大模型用于推荐系统-论文阅读 - 韩恪的文章 - 知乎 https://zhuanlan.zhihu.com/p/673491147
26. 浅谈推荐算法之长序列模型TWIN - feng的文章 - 知乎 https://zhuanlan.zhihu.com/p/667311200
27. PyTorch Parallelism - talk notes - JackonYang的文章 - 知乎 https://zhuanlan.zhihu.com/p/677808809
28. 最小熵原理（六）：词向量的维度应该怎么选择：https://kexue.fm/archives/7695
29. 维度公式可用性分析：https://kexue.fm/archives/8711
30. Havenask在线检索服务：https://github.com/alibaba/havenask
31. 如何入门 GPT 并快速跟上当前的大语言模型 LLM 进展？ - 叶兀的回答 - 知乎 https://www.zhihu.com/question/599713780/answer/3018222382


## 框架安装
```
初学者建议统一安装cpu版本

安装pytorch
http://arthurchiao.art/blog/gpt-as-a-finite-state-markov-chain-zh/#21-%E5%AE%89%E8%A3%85-pytorch

pip3 install torch torchvision -i https://pypi.mirrors.ustc.edu.cn/simple # 用国内源
pip3 install graphviz -i https://pypi.mirrors.ustc.edu.cn/simple


安装paddlepaddle全家桶
pip3 install paddlepaddle==2.5.1 -i https://pypi.tuna.tsinghua.edu.cn/simple    
pip3 install paddleocr
```

### 前置安装
```bash
apt install python3.9
apt install python3-pip
# openGL库
apt-get install libgl1-mesa-glx


```

### transformers 安装
```bash
安装rust
curl https://sh.rustup.rs -sSf | sh
# rust生效
source "$HOME/.cargo/env"

pip3 install setuptools_rust
pip3 install transformers

```

### juypter 安装
```
pip install --upgrade pip
pip3 install --upgrade pip
pip3 install --upgrade pip3

pip3 install setuptools_scm
pip3 install argon2-cffi-bindings
pip3 install jupyter

```

### 镜像安装
```
阿里云
https://cr.console.aliyun.com/cn-shanghai/instances/artifact

cpu
docker pull dsw-registry.cn-hangzhou.cr.aliyuncs.com/pai/pytorch:1.8-cpu-py36-ubuntu18.04

gpu
dsw-registry.cn-hangzhou.cr.aliyuncs.com/pai/pytorch:1.8PAI-gpu-py36-cu101-ubuntu18.04

sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 100 --slave /usr/bin/g++ g++ /usr/bin/g++-9


```

## 工具向
### 翻译工具
```
训练Transformer 模型将 中文翻译成英语：https://zhuanlan.zhihu.com/p/469388563

```

## 数据处理
```
paddle：https://paddlenlp.readthedocs.io/zh/latest/data_prepare/data_preprocess.html



```





## Embedding
### 介绍
```
向量检索引擎：https://zhuanlan.zhihu.com/p/364923722
IndexFlatL2（基于欧氏距离的暴力索引）
IndexIVFFlat（加聚类的倒排索引，支持欧式距离和向量内积两种距离算法）
IndexIVFPQ（加聚类、加量化的倒排索引）

开源项目
facebook：https://github.com/facebookresearch/faiss
阿里：https://github.com/alibaba/proxima

手把手搭建一个语义检索系统：https://github.com/PaddlePaddle/PaddleNLP/tree/develop/applications/neural_search

OpenAI官方教程：如何使用基于embeddings检索来解决GPT无法处理长文本和最新数据的问题 - 数据学习的文章 - 知乎
https://zhuanlan.zhihu.com/p/622365401

https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb

```
### 模型和工业实践
```
Embedding技术的两个小应用：鲁迅全集检索 & 新闻早报聚类 - 段誉的文章 - 知乎
https://zhuanlan.zhihu.com/p/672400191

```
### 标签 转 embedding
```


```

### 数值 转 embedding
```


```
### 文字 转 embedding
```


```
### 数学公式 转 embedding
```


```
### pdf 转 embedding
```
pdf文本加载
https://zhuanlan.zhihu.com/p/644938147

paddleOCR解析文本：https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.7/ppstructure/docs/quickstart.md



安装ocr工具
yum install mupdf-devel


paddleocr --image_dir=/home/aistudio/data/pdf/IETS13_A.pdf --type=structure --recovery=true --use_pdf2docx_api=true

paddleocr --image_dir=/home/aistudio/data/pdf/IETS13_A.pdf --type=structure --recovery=true --lang='en'

paddleocr --image_dir=/home/aistudio/data/pdf/MOBIUS\ Towards\ the\ Next\ Generation\ of\ Query-Ad\ Matching\ in\ Baidu\ Sponsored\ Search.pdf --type=structure --recovery=true --lang='en'



```
### 图片 转 embedding
```


```
### 语音 转 embedding
```


```

### 语音
```bash
https://aistudio.baidu.com/aistudio/projectdetail/4353348?sUid=2470186&shared=1&ts=1660878142250

# 前置依赖
apt-get update
apt install sudo -y
# programm
apt install -y gcc-9 g++-9
apt install -y python3.8
apt install -y python3-pip

apt-get install libsndfile1 -y
apt-get install ffmpeg -y


# 新增speech环境
git clone https://gitee.com/mirrors/pyenv.git ~/.pyenv
export PYTHON_BUILD_MIRROR_URL="https://npm.taobao.org/mirrors/python/"
export HOME=~
export PYENV_ROOT="$HOME/.pyenv"
export PATH="$PYENV_ROOT/bin:$PATH"

pyenv virtualenv 3.8.10 speech 
pyenv activate speech
source speech/bin/activate
pip3 install paddlepaddle==2.4.2 -i https://mirrors.aliyun.com/pypi/simple/
pip3 install paddlespeech -i https://mirrors.aliyun.com/pypi/simple/
pip3 install paddlespeech_ctcdecoders -i https://mirrors.aliyun.com/pypi/simple/
# mp3 to wav 格式
pip3 install pydub -i https://mirrors.aliyun.com/pypi/simple/

# 退出
pyenv deactivate


paddlespeech asr --lang zh --input /docker/root/projects/demo/data/mp4/剑4真题听力/test1/test1_section1.wav


```

## 检索系统
### ES安装
```
官网：https://www.elastic.co/cn/downloads/elasticsearch




━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
✅ Elasticsearch security features have been automatically configured!
✅ Authentication is enabled and cluster connections are encrypted.

ℹ️  Password for the elastic user (reset with `bin/elasticsearch-reset-password -u elastic`):
  V-W=e0BF3SBK+a2mspgU

ℹ️  HTTP CA certificate SHA-256 fingerprint:
  5c75487e8bb0c8126f493257e6772e33a17bc763b64096886b0246ca853483b5

ℹ️  Configure Kibana to use this cluster:
• Run Kibana and click the configuration link in the terminal when Kibana starts.
• Copy the following enrollment token and paste it into Kibana in your browser (valid for the next 30 minutes):
  eyJ2ZXIiOiI4LjEwLjQiLCJhZHIiOlsiMTEuMy45Ljk0OjkyMDAiXSwiZmdyIjoiNWM3NTQ4N2U4YmIwYzgxMjZmNDkzMjU3ZTY3NzJlMzNhMTdiYzc2M2I2NDA5Njg4NmIwMjQ2Y2E4NTM0ODNiNSIsImtleSI6IlhWWlJWb3NCaFlzTUZYWWZCNmNoOkV6WExwOTFoVGh5N1hxUUVzNVIyQXcifQ==

ℹ️  Configure other nodes to join this cluster:
• On this node:
  ⁃ Create an enrollment token with `bin/elasticsearch-create-enrollment-token -s node`.
  ⁃ Uncomment the transport.host setting at the end of config/elasticsearch.yml.
  ⁃ Restart Elasticsearch.
• On other nodes:
  ⁃ Start Elasticsearch with `bin/elasticsearch --enrollment-token <token>`, using the enrollment token that you generated.
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━


```
### 长文本
```
大模型检索增强生成（RAG）有哪些好用的技巧？ - 战士金的回答 - 知乎
https://www.zhihu.com/question/625481187/answer/3260085982




```


#### case 1
```
你现在是一个关系提取的大模型，我会输入一段文本给你
    输入结构
    1. 背景
    2. 问题
    输出结构
    1. 三元组<实体，属性名，属性值>
    2. 如果你不知道答案，请不要打多余的字，直接回复：没有找到该问题对应的知识
    
    背景如下
    <
    科列斯尼科夫,别名,科列斯尼科夫
科列斯尼科夫,外文名,Kolesnikov
科列斯尼科夫,国籍,俄罗斯
科列斯尼科夫,出生日期,1985年12月26日
科列斯尼科夫,身高,195CM
科列斯尼科夫,体重,78kg
科列斯尼科夫,运动项目,篮球
科列斯尼科夫,所属运动队,圣彼得堡俱乐部
科列斯尼科夫,中文名,科列斯尼科夫
科列斯尼科夫,位置,后卫
科列斯尼科夫,号码,5号
科列斯尼科夫,出生地,俄罗斯
科列斯尼科夫,性别,男
科列斯尼科夫,出生年月,1985年12月26日
科列斯尼科夫,星座,魔羯座
科列斯尼科夫,职业,篮球
米哈伊尔·彼特罗维奇·科列斯尼科夫,别名,米哈伊尔·彼特罗维奇·科列斯尼科夫
米哈伊尔·彼特罗维奇·科列斯尼科夫,中文名,米哈伊尔·彼特罗维奇·科列斯尼科夫
米哈伊尔·彼特罗维奇·科列斯尼科夫,外文名,Михаи́л Петро́вич Коле́сников
米哈伊尔·彼特罗维奇·科列斯尼科夫,出生日期,1939.10
米哈伊尔·彼特罗维奇·科列斯尼科夫,逝世日期,2007.3.26
同第三帝国决斗,作者,玛·瓦·科列斯尼科娃 / 米·谢·科列斯尼科夫
>
    问题如下
    <我想了解一下科列斯尼科夫这位篮球运动员。我知道他是一位后卫球员，且运动技能很高。但是他的出生年月，以及他在哪个队伍效力，他的身高和体重等等，这些我都不太清楚。你能告诉我这些信息吗？>
    输出如下
    输出结果以三元组形式返回: <实体，属性名，属性值>




你现在是一个关系提取的大模型，我会输入一段文本给你
    输入结构
    1. 背景
    2. 问题
    输出结构
    1. 三元组<实体，属性名，属性值>
    2. 如果你不知道答案，请不要打多余的字，直接回复：没有找到该问题对应的知识
    
    背景如下
    <
许仕廉,别名,许仕廉
许仕廉,中文名,许仕廉
许仕廉,国籍,中国
许仕廉,出生地,湖南湘潭
许仕廉,出生日期,1896年
许仕廉,毕业院校,爱荷华大学
许仕廉,主要成就,中国著名社会学家
许仕廉,主要从事,社会、人口学的调查研究教学工作
许仕廉,籍贯,湖南湘潭
许仕廉,性别,男
许仕廉,民族,汉族
许仕廉,出生年月,1896年
许仕廉,职业,社会学家
许仕廉,代表作品,《文化与政治》；《一个市镇调查的尝试》；《社会教育与社会理论》；《中国人口问题》；《人口论纲要》
许仕廉,国 籍,中国
>
    问题如下
    <许仕廉主要是任职什么的？>
    输出如下
    输出结果以三元组形式返回: <实体，属性名，属性值>


```
### faiss
```
看faiss源码解析

https://github.com/facebookresearch/faiss/wiki

faiss使用-入门级小白篇代码教程 - 程序员小丁的文章 - 知乎
https://zhuanlan.zhihu.com/p/642959732

向量数据库入坑指南：初识 Faiss，如何将数据转换为向量
https://cloud.tencent.com/developer/article/2153613

pip install  -i https://mirrors.aliyun.com/pypi/simple faiss-cpu 
pip install  -i https://mirrors.aliyun.com/pypi/simple  faiss-gpu





```

### puck
```
https://github.com/baidu/puck

sudo apt-get remove cmake
apt-get install -y libssl-dev
wget https://github.com/Kitware/CMake/releases/download/v3.21.0/cmake-3.21.0.tar.gz
tar -zxvf cmake-3.21.0.tar.gz
cd cmake-3.21.0
./bootstrap -DCMAKE_USE_OPENSSL=OFF
make -j8  && make install
cmake --version




```

### query理解
```
搜索召回算法实践：文本召回综述 - 南枫的文章 - 知乎
https://zhuanlan.zhihu.com/p/467939766
深入理解搜索引擎——详解query理解 - 药老算法的文章 - 知乎
https://zhuanlan.zhihu.com/p/344631739
美团搜索——基于用户Session的Query改写 - 药老算法的文章 - 知乎
https://zhuanlan.zhihu.com/p/355132926



query结构分析
query改写
query纠错、query对齐、query扩展

```

### 搜索召回
```
Que2Search（上）：FaceBook新一代query搜索召回模型分享 - MECH的文章 - 知乎
https://zhuanlan.zhihu.com/p/615284379
深入理解搜索引擎-搜索召回 - 药老算法的文章 - 知乎
https://zhuanlan.zhihu.com/p/348159133
Que2Search（下）：OPPO搜索广告召回模型落地分享 - MECH的文章 - 知乎
https://zhuanlan.zhihu.com/p/616880233
[召回|KDD2020|FaceBook]Embedding-based Retrieval in Facebook Search论文超级详细解读 - 杰尼小子的文章 - 知乎
https://zhuanlan.zhihu.com/p/438047408

经典推荐算法学习（十三）| 常见推荐召回算法梳理 - 秋雨淅淅l的文章 - 知乎 https://zhuanlan.zhihu.com/p/472770659


```


### 文本相关性
```
百度搜索相关性算法笔记 - 张备的文章 - 知乎
https://zhuanlan.zhihu.com/p/586676631
知乎搜索文本相关性与知识蒸馏 - DataFunTalk的文章 - 知乎
https://zhuanlan.zhihu.com/p/422185499

```
## NLP - 纠错
### 资料
1. 中文纠错技术综述 - 低级炼丹师的文章 - 知乎 https://zhuanlan.zhihu.com/p/357812484
2. 百度中文纠错技术：https://mp.weixin.qq.com/s/r0kWgPHKthPgGqTbVc3lKw
3. PyCorrector文本纠错工具实践和代码详解 - Roger的文章 - 知乎 https://zhuanlan.zhihu.com/p/138981644

### 信息抽取

### BM25
1. BM25算法的通俗理解 - 徐波的文章 - 知乎 https://zhuanlan.zhihu.com/p/420048609
2. 

### ernie
1. 文本分类
2. 命名实体识别
3. 超详细中文预训练模型ERNIE使用指南 - 飞桨PaddlePaddle的文章 - 知乎 https://zhuanlan.zhihu.com/p/76757794
4. 


### ernie_vil 

### 蒸馏技术

https://aistudio.baidu.com/modelsdetail/21/intro?modelId=21
ERNIE-Tiny: A Progressive Distillation Framework for Pretrained Transformer Compression
ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation

### TTS
```
paddleSpeech
https://paddlespeech.readthedocs.io/en/latest/tts/demo.html

```


## 模型推理预估
```
纯原生 PyTorch 加速生成式 AI 模型
代码地址：https://github.com/pytorch-labs/gpt-fast

mamba
https://github.com/state-spaces/mamba.
论文链接：https://arxiv.org/ftp/arxiv/papers/2312/2312.00752.pdf

llama.c
https://github.com/karpathy/llama2.c

Xinference
https://zhuanlan.zhihu.com/p/663065437


llamacpp


chatglm
https://github.com/li-plus/chatglm.cpp

地址标准化服务AI深度学习模型推理优化实践 - 阿里云云栖号的文章 - 知乎
https://zhuanlan.zhihu.com/p/552908554

大模型推理加速论文阅读（三） - 风不语的文章 - 知乎
https://zhuanlan.zhihu.com/p/676309607


大模型如何高效部署？CMU最新万字综述纵览LLM推理MLSys优化技术 - Hsword的文章 - 知乎
https://zhuanlan.zhihu.com/p/677635306


大模型推理加速调研（框架、方法） - mingming的文章 - 知乎
https://zhuanlan.zhihu.com/p/683986024


flash attention V1 V2 V3 V4 如何加速 attention - RedHerring的文章 - 知乎
https://zhuanlan.zhihu.com/p/685020608

```

### 前沿关键词
- FlashAttention、Flash Attention v2、Flash-Decoding的作者
- Mamba
- llama.c
- llamacpp

### 加速思路
1. 量化
2. 权重量化
3. kv cache  int8量化

## 模型评估
```
平均绝对误差（MAE）
均方误差（MSE）
标准化加权均方根对数误差（NWRMSLE）
交叉熵

```

## 模型结构
```

Mixtral 8x7B(Mistral MoE) 模型解析 - CodeLearner的文章 - 知乎
https://zhuanlan.zhihu.com/p/684922663



```

## GPT
### 制作一个BadyGpt
```
参考文档：http://arthurchiao.art/blog/gpt-as-a-finite-state-markov-chain-zh/#21-%E5%AE%89%E8%A3%85-pytorch

langchain方式
https://github.com/chatchat-space/langchain-ChatGLM

```

### chatGLM
```
https://github.com/RonaldJEN/FinanceChatGLM.git



比赛
官网论坛：https://tianchi.aliyun.com/competition/entrance/532126/forum
全训练过程：https://tianchi.aliyun.com/forum/post/573555
数据集：https://modelscope.cn/datasets/modelscope/chatglm_llm_fintech_raw_dataset/summary?spm=a2c22.21852664.0.0.68171434jkTm0x
baseline代码：https://github.com/RonaldJEN/FinanceChatGLM/
参赛笔记：https://tianchi.aliyun.com/forum/post/571708
基于paddlepaddle的chatglm推理实现代码：https://tianchi.aliyun.com/forum/post/572601

微调模型：https://github.com/THUDM/ChatGLM-6B/tree/main/ptuning
paddle结合GLM：https://aistudio.baidu.com/aistudio/projectdetail/6195067
动手搭建一套端到端文本语义检索系统：https://aistudio.baidu.com/projectdetail/3351784

思路 
gpt生成关键词 prompt 区分文本分析 数学计算
先测试分类效果 
请将下述问题区分为搜索信息类，计算数值类，总结陈述类

关键词语义搜索找到相关pdf paddle语义召回
5000行构建花了10分钟 cpu

pdf识别语义



解析pdf逻辑参考
Traceback (most recent call last):
  File "paddle_model.py", line 16, in <module>
    pprint(docprompt([{"doc": pdf_path, "prompt": ["财报利润率多少",  "住宅投资是多少?"]}]))
  File "/usr/local/lib/python3.8/dist-packages/paddlenlp/taskflow/taskflow.py", line 850, in __call__
    results = self.task_instance(inputs)
  File "/usr/local/lib/python3.8/dist-packages/paddlenlp/taskflow/task.py", line 515, in __call__
    inputs = self._preprocess(*args)
  File "/usr/local/lib/python3.8/dist-packages/paddlenlp/taskflow/document_intelligence.py", line 90, in _preprocess
    ocr_result = self._ocr.ocr(example["doc"], cls=True)
  File "/usr/local/lib/python3.8/dist-packages/paddleocr/paddleocr.py", line 637, in ocr
    img = check_img(img)
  File "/usr/local/lib/python3.8/dist-packages/paddleocr/paddleocr.py", line 527, in check_img
    img, flag_gif, flag_pdf = check_and_read(image_file)
  File "/usr/local/lib/python3.8/dist-packages/paddleocr/ppocr/utils/utility.py", line 96, in check_and_read
    for pg in range(0, pdf.pageCount):


下载模型和数据集
参考链接：https://tianchi.aliyun.com/forum/post/573555
pip3 install datasets==2.13.0
pip3 install dill==0.3.6
pip3 install multiprocess==0.70.14
pip3 install numpy pandas urllib3
pip3 install torch
pip3 install transformers -U
pip3 install accelerate -U
pip3 install sentencepiece -U
pip3 install torch torchvision -U
pip3 install modelscope -U 

打包当前环境
pip3 freeze > requirements.txt

数据集
git clone http://www.modelscope.cn/datasets/modelscope/chatglm_llm_fintech_raw_dataset.git

异常
RuntimeError: "addmm_impl_cpu_" not implemented for 'Half'
原因
机器没有gpu，需要配置只跑cpu。加参数 trust_remote_code=True
model = Model.from_pretrained('ZhipuAI/chatglm2-6b', low_cpu_mem_usage=False, trust_remote_code=True, revision='v1.0.7')



export PYTHONPATH=$PYTHONPATH:/docker/root/projects/demo/project/chatglm/baseline_demo/FinanceChatGLM

总结
大模型现阶段一些问题和优势
问题：
模型需要的资源太多了，响应慢，开发慢。这些都是成本
模型对数据质量要求很高，不然基本不可用
输出结果不可控
不适合做大数据计算
不适合做搜索引擎

优势
在提供足量的正确的数据，能融合数据，并生成可看的结果

大模型 + 向量 + 搜索 + 大数据
数据存储成本
索引构建成本
向量生成成本
大模型运行成本
每一个成本都很高
而且pipeline很长
数据同步延迟高，排查问题慢，同时依赖没有简化现有的软件开发模式，反而新增一个链路

我理想中认为的大模型
存储所有的大数据内容，完成端到端的数据链路更新

实际中，因为大模型的成本问题
大模型应该需要具备推理和分析能力，来代替人，生成sql，生成代码。然后提交执行命令
完成一系列自动化机器运维，软件开发，bug修复等操作
这样就能带来明显的价值效益


```
### chatglm3-本地部署
```
https://modelscope.cn/models/Xorbits/chatglm3-ggml/summary

高性能推理

mac
./Library/Python/3.9/bin/xinference -p 9997

```

### AI小镇
```
https://github.com/joonspk-research/generative_agents

```

## huggingface
### SkyTextTiny 模型
```
git clone https://huggingface.co/SkyWork/SkyTextTiny.git    


```
## 业务向
### Readlist
1. 分割一切：https://pytorch.org/blog/accelerating-generative-ai/
2. 

### 智能文档信息提取
```
https://www.paddlepaddle.org.cn/support/news?action=detail&id=3174

DocVQA榜单
https://rrc.cvc.uab.es/?ch=17&com=evaluation&task=1 

百度AI开放平台——智能文档分析平台
https://ai.baidu.com/tech/nlp/Textanalysis

在线调试
https://console.bce.baidu.com/tools/#/api?product=AI&project=%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB&parent=%E9%89%B4%E6%9D%83%E8%AE%A4%E8%AF%81%E6%9C%BA%E5%88%B6&api=oauth%2F2.0%2Ftoken&method=post


开源了！文心大模型ERNIE-Tiny轻量化技术，又准又快，效果全开 - 飞桨PaddlePaddle的文章 - 知乎
https://zhuanlan.zhihu.com/p/535541528

【快速上手ERNIE 3.0】机器阅读理解实战 - 快速实现AI想法的文章 - 知乎
https://zhuanlan.zhihu.com/p/536541088


```
1. 信息抽取方法综述：https://zhuanlan.zhihu.com/p/376898772
2. 




### 2023IKCEST第五届一带一路国际大数据竞赛
#### todolist
1. 提供resnet模型效果
2. bert ernie模型训练加载器
3. LinearDecayWithWarmup 学习率调度器类，它根据线性递减策略计算学习率。在训练过程中
4. LinearWarmup
5. 交叉熵损失
6. 评估的时候采用准确率指标
7. Optimizer
   1. Momentum
   2. adam
8. 模型断点训练
9.  paddle
   1. vision
   2. paddle.concat
   3. paddle.stack
   4. paddle.reshape
   5. 

#### 优化目标
```
文本识别
图形识别
向量concat

预测分类

我的想法
文本识别的特征如何 和 图像识别的特征保持一致呢

数据增广 丰富训练集，加强训练
https://ai.baidu.com/ai-doc/ERNIE-Ultimate/Pl6egw3pu


可使用工具
PaddleDetection

分词工具与词表生成工具
数据增强
交叉验证
网格搜索
编码及转换工具
https://github.com/PaddlePaddle/ERNIE/tree/ernie-kit-open-v1.0/applications/tools



问题拆解
文本分类
https://github.com/PaddlePaddle/ERNIE/tree/ernie-kit-open-v1.0/applications/tasks/text_classification

文本匹配
https://github.com/PaddlePaddle/ERNIE/tree/ernie-kit-open-v1.0/applications/tasks/text_matching


序列标注
https://github.com/PaddlePaddle/ERNIE/tree/ernie-kit-open-v1.0/applications/tasks/sequence_labeling


信息抽取
https://github.com/PaddlePaddle/ERNIE/tree/ernie-kit-open-v1.0/applications/tasks/information_extraction_many_to_many

文本生成
https://github.com/PaddlePaddle/ERNIE/tree/ernie-kit-open-v1.0/applications/tasks/text_generation

数据蒸馏
https://github.com/PaddlePaddle/ERNIE/tree/ernie-kit-open-v1.0/applications/tasks/data_distillation
Distilling Task-Specific Knowledge from BERT into
Simple Neural Networks：https://arxiv.org/pdf/1903.12136.pdf

指标评估
https://ai.baidu.com/ai-doc/ERNIE-Ultimate/nkmlroqy2

多模态
自回归网络

自编码网络

Token-Drop
https://arxiv.org/pdf/2106.14448v1.pdf

paddlenlp工具


paddlecv



```
#### 数据集介绍
```
标签说明
0	1	2
non-rumor	rumor	unverified


img文件夹下存放每一条声明的图片

img_html_news文件夹下存放根据每一条声明的caption检索到的网页与图片，其中direct_annotation.json包含如下信息：

{
      "img_link": 检索到的相关图片的链接,
      "page_link": 检索到的网页链接,
      "domain": 检索到的网页的域名,
      "snippet": 检索到的网页的简洁摘要,
      "image_path": 检索到的图片的路径,
      "html_path": 检索到的网页的路径,
      "page_title": 检索到的网页标题
}
inverse_search文件夹下存放根据声明的图片找到的网页，其中inverse_annotation.json包含如下信息

{
"entities": 声明中图片中的实体, 
"entities_scores": 声明中图片中的实体的分数, 
"best_guess_lbl": 声明中图片最可能是什么, 
"all_fully_matched_captions": , 
"all_partially_matched_captions":  
"fully_matched_no_text": 
上述三个字段的值均为寻找到的网页，为一个列表，列表中的元素为一个字典，格式如下
	{
	"page_link": 检索到的网页链接, 
	"image_link": 检索到的图片链接, 
	"html_path": 检索到的网页的路径, 
	"title": 检索到的网页的标题
	}
}








```
建模
1. 问题分析
   1. 输入 陈述句
   2. 输出 判断真假消息或不确定
2. 问题拆解
   1. 定义 真消息
      1. 
   2. 定义 假消息
   3. 定义 不确定消息


模型库
1. DUMA 给定一段上下文Passage, 问题Question, 选项Answer Options，选出最合适的答案。
2. roberta 文本向量化
3. RoBERTa-wwm-large
4. ernie模型历史迭代
   1. https://ai.baidu.com/ai-doc/ERNIE-Ultimate/5kye50810
5. 跨模态检索
   1. https://ai.baidu.com/ai-doc/ERNIE-Ultimate/ukxk3hkzc
6. 跨模态信息抽取
   1. https://ai.baidu.com/ai-doc/ERNIE-Ultimate/nkwnlv73o
7. 模型库评测
   1. nlp：https://github.com/CLUEbenchmark/CLUE
8. 工程
   1. ERNIEKit：https://ai.baidu.com/ai-doc/ERNIE-Ultimate/rkmlroren  
   2. paddleNLP
      1. task api：https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples


论文库
1. 训练
   1. 基于 In-batch negatives 策略训练：https://arxiv.org/abs/2004.04906
   2. 基于 HardestNeg 策略训练

文本数据处理
1. 计算下最长句子的长度
2. 生成向量方案
   1. roberta 模型
3. 文本特征方案
   1. ernie-m
   2. bert

图形数据处理
1. resnet

#### 训练速度
```
杀死进程
ps -ef|grep multi_gpu.py | awk '{print $2}' |  xargs kill -9

多卡和单卡情况差不多
global step 100, epoch: 1, batch: 100, loss: 0.94645, accu: 0.41750, speed: 0.09 step/s
global step 200, epoch: 1, batch: 200, loss: 1.37231, accu: 0.43625, speed: 0.09 step/s
global step 300, epoch: 1, batch: 300, loss: 1.22456, accu: 0.45083, speed: 0.10 step/s
global step 400, epoch: 1, batch: 400, loss: 1.50505, accu: 0.47937, speed: 0.10 step/s
global step 500, epoch: 1, batch: 500, loss: 0.68333, accu: 0.50050, speed: 0.10 step/s

单卡会出现内存溢出，训练失败的情况

```

#### 模型训练优化经验
```
如何优化你的模型 - 错乱空时的文章 - 知乎
https://zhuanlan.zhihu.com/p/360647927

NLP中文预训练模型泛化能力挑战赛
https://tianchi.aliyun.com/competition/entrance/531841/forum

2021语言与智能技术竞赛：多形态信息抽取任务
https://aistudio.baidu.com/competition/detail/65/0/introduction

二等奖方案|产品评论观点提取赛题
https://discussion.datafountain.cn/articles/detail/3764

推荐 nlp cv大模型参数量总结
https://zhuanlan.zhihu.com/p/529863941

竞赛达人的竞赛之旅
https://zhuanlan.zhihu.com/p/375688838

数据
数据预处理
数据增广
外部数据
模型
大力出奇迹
模型结构
loss
label Smooting
优化方法
学习率
对抗训练
EMA，SWA
正则化
word mixup
dropout
early stop
后处理
阈值优化
其他
未起效方法
训练技巧
自知识蒸馏


```
### GPT解决数学建模问题
```


```


### 西安电子科技大学-基于文心一言的法律助手
```
1、提炼核心需求

2、思考满足核心需求的方式

3、评估方式优劣选定方案

4、思考功能概要

5、思考支撑功能和关联功能

6、细化设计功能

7、子功能（功能间迭代）



prd
https://www.woshipm.com/pmd/4289732.html

```
1. 法治的重要性 - 背景
2. 法治遇到的问题 - 调研
   1. 统计角度
      1. 全民普法率
      2. 冤家错案
      3. 法考难度
   2. 执法效率
   3. 
3. 执法链路参与人
   1. 法律人角度：学法，懂法，立法
   2. 群众角度：普法重要性
   3. 原告被告：维护自身权益重要性
4. 法律
   1. 刑法＞刑诉＞行政法＞法理学＞民法＞民诉
5. 产品
   1. 业界情况对比
      1. 差异化
      2. 竞品情况
   2. 整体流程
   3. 需求描述
   4. 版本规划
   5. 产品框架
   6. 功能列表
6. 功能需求
7. 非功能需求
   1. 安全
8. 应用场景
9. 市场化
10. 未来
    1.  视频多模态

   


### 信贷时序特征建模
```
信贷时序数据与特征工程介绍 - 求是汪在路上的文章 - 知乎
https://zhuanlan.zhihu.com/p/397614923


```

### 线上线下全场景生鲜超市库存履约一体化决策
```
https://github.com/MineQihang/BDCI2023/tree/main
```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 自然场景文字定位技术详解
```
自然场景文字定位是文字识别中非常重要的一部分。与通用的物体检测相比，文字定位更具挑战性，文字在长宽比、尺度和方向上有更大范围的变化。针对这些问题，本文介绍一种融合文字片段及金字塔网络的场景文字定位方法。该方法将特征金字塔机制应用到单步多框检测器以处理不同尺度文字，同时检测多个文字片段以及学习出文字片段之间8-neighbor连接关系，最后通过8-neighbor连接关系将文字片段连接起来，实现对不同方向和长宽比的文字定位。此外，针对文字通常较小特点，扩大检测网络中backbone模型深层特征图，以获得更好性能。

（1）基于分割的文本定位；（2）基于回归的文本定位。

"中文门脸招牌文字识别"比赛（ICDAR 2019 Robust Reading Challenge on Reading Chinese Text on Signboards）。


刘曦，美团视觉图像中心文字识别组算法专家
```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

## 竞赛方案复现
```
关键词搜索
名方案

竞赛达人竞赛之旅
https://zhuanlan.zhihu.com/p/375688838


“公益AI之星”挑战赛-新冠疫情相似句对判定大赛
https://tianchi.aliyun.com/competition/entrance/231776/forum
第一名
https://tianchi.aliyun.com/notebook/101624

NLP中文预训练模型泛化能力挑战赛
https://tianchi.aliyun.com/competition/entrance/531841/forum

2021语言与智能技术竞赛：多形态信息抽取任务
https://aistudio.baidu.com/competition/detail/65/0/introduction

二等奖方案|产品评论观点提取赛题
https://discussion.datafountain.cn/articles/detail/3764

表格检测
https://aistudio.baidu.com/competition/detail/702/0/submit-result
https://aistudio.baidu.com/competition/detail/704/0/introduction

第二名方案
https://aistudio.baidu.com/projectdetail/5398861?channelType=0&channel=0


百度网盘AI大赛——图像处理挑战赛：文档图像摩尔纹消除第1名方案
https://aistudio.baidu.com/projectdetail/3462083?channelType=0&channel=0


百度网盘文档图像超分比赛-Serendipity团队-AB榜第二方案 metaRLFN网络
https://aistudio.baidu.com/projectdetail/5133608?channelType=0&channel=0


第五届“中国法研杯”司法人工智能挑战赛
https://aistudio.baidu.com/competition/detail/664/0/submit-result

中国人工智能大赛·语言与知识技术竞赛（个人赛）冠军方案分享
https://aistudio.baidu.com/projectdetail/755387?channelType=0&channel=0

WSDM Cup 2023: Pre-training for Web Search Demo Code
https://aistudio.baidu.com/projectdetail/4844225?channelType=0&channel=0

【飞桨学习赛：百度搜索首届技术创新挑战赛：赛道一】第5名方案
https://aistudio.baidu.com/projectdetail/4950227?channelType=0&channel=0


百度搜索首届技术创新挑战赛赛道二Baseline
https://aistudio.baidu.com/projectdetail/5007642?channelType=0&channel=0


“飞桨杯”重庆市首届人工智能创新大赛-社交网络大数据谣言核查官方baseline
https://aistudio.baidu.com/projectdetail/4913037?channelType=0&channel=0

2022 IKCEST第四届“一带一路”国际大数据竞赛：“一带一路”重点语种-法俄泰阿与中文互译
https://aistudio.baidu.com/competition/detail/477/0/introduction

2022 CCF BDCI 基于文心NLP大模型的阅读理解可解释评测
https://aistudio.baidu.com/competition/detail/394/0/introduction
第五名
https://aistudio.baidu.com/projectdetail/5016503?channelType=0&channel=0


CCKS2022基于知识图谱的优质文章识别
https://aistudio.baidu.com/competition/detail/255/0/introduction


2022 CCF BDCI 基于文心CV大模型的智慧城市视觉多任务识别
https://aistudio.baidu.com/competition/detail/455/0/introduction
第一名
https://aistudio.baidu.com/projectdetail/5035322?channelType=0&channel=0


AIWIN 世界人工智能创新大赛：中文保险小样本多任务竞赛
https://aistudio.baidu.com/competition/detail/218/0/introduction

AIWIN 世界人工智能创新大赛：发债主体违约风险预测竞赛
https://aistudio.baidu.com/competition/detail/222/0/introduction

2021“智荟杯”浦发百度高校极客挑战赛
https://aistudio.baidu.com/competition/detail/123/0/task-definition

【Paddle打比赛】基于PaddleNLP法研杯2022 -犯罪事实实体识别
https://aistudio.baidu.com/projectdetail/4821353?channelType=0&channel=0

2021中国软件杯——新闻智分系统
https://aistudio.baidu.com/projectdetail/1981601?channelType=0&channel=0


2022 CCF BDCI 模心智创-文心大模型智能创意赛
https://aistudio.baidu.com/competition/detail/397/0/introduction



文本智能校对大赛
https://aistudio.baidu.com/competition/detail/404/0/introduction

航旅纵横-领域知识问答测评
https://aistudio.baidu.com/competition/detail/313/0/introduction

飞桨论文复现挑战赛（第六期）
https://aistudio.baidu.com/competition/detail/205/0/introduction

飞桨论文复现挑战赛（第七期）
https://aistudio.baidu.com/competition/detail/406/0/task-definition

2022世界人工智能大会黑客马拉松：百度飞桨论文复现赛
https://aistudio.baidu.com/competition/detail/430/0/introduction

2022世界人工智能大会黑客马拉松：百度飞桨黑客马拉松
https://aistudio.baidu.com/competition/detail/428/0/introduction

兴智杯 全国人工智能创新应用大赛：百度飞桨论文复现赛
https://aistudio.baidu.com/competition/detail/439/0/introduction

“兴智杯”全国人工智能创新应用大赛：深度学习模型可解释性赛
https://aistudio.baidu.com/competition/detail/472/0/introduction



```
### Kaggle汇总
```
前端搜索：https://github.com/faridrashidi/kaggle-solutions

```
### Kaggle - mercari-price-suggestion-challenge
```
比赛链接：https://www.kaggle.com/c/mercari-price-suggestion-challenge

解决方案：https://www.leiphone.com/category/yanxishe/HGSuMdM6c4U6jWLi.html

代码：https://github.com/pjankiewicz/mercari-solution

```

### Kaggle - Predict Future Sales
```
比赛链接：https://www.kaggle.com/competitions/competitive-data-science-predict-future-sales/data

代码：https://www.kaggle.com/code/zhangyunsheng/xgboost/notebook

```

### 2018 腾讯广告算法大赛
```
10th:https://github.com/ShawnyXiao/2018-Tencent-Lookalike

```

### 2021 腾讯广告算法大赛
```
https://github.com/JacksonWuxs/taac2021-Video-Classification-Rank5

```



### Kaggle - Corporacion Favorita Grocery Sales Forecasting
```
比赛地址：

https://www.kaggle.com/c/favorita-grocery-sales-forecasting

论文地址：

https://arxiv.org/pdf/1803.04037.pdf

方案：https://cloud.tencent.com/developer/article/1166628
代码：https://www.kaggle.com/code/shixw125/1st-place-lgb-model-public-0-506-private-0-511/script
```

### 数据科学竞赛2019
```
数据科学竞赛2019：https://mp.weixin.qq.com/s?__biz=Mzk0NDE5Nzg1Ng==&mid=2247490157&idx=1&sn=674461f9cbb0e60bf23994576b67c5d8&source=41#wechat_redirect

【乘用车细分市场销量预测】

赛事方向：预测回归、数据挖掘

赛事简介：本赛题需要参赛队伍根据给出的60款车型在22个细分市场（省份）的销量连续24个月（从2016年1月至2018年12月）的销量数据，建立销量预测模型；基于该模型预测同一款车型和相同细分市场在接下来一个季度连续4个月份的销量；除销量数据外，还提供同时期的用户互联网行为统计数据，包括：各细分市场每个车型名称的互联网搜索量数据；主流汽车垂直媒体用户活跃数据等。参赛队伍可同时使用这些非销量数据用于建模。除了模型的准确性外，参赛队伍需对本赛题任务有系统性的思考和设计，在决赛阶段，参赛队伍对于所提交的模型的适应性、可扩展性、代码的工程性等方面也会影响参赛队伍的最终名次。

方案分享：

https://mp.weixin.qq.com/s/-tT9BKrANTwJK9-N1K4j9g



【消费者人群画像—信用智能评分】

赛事方向：机器学习、数据挖掘

赛事简介：中国移动福建公司提供2018年x月份的样本数据（脱敏），包括客户的各类通信支出、欠费情况、出行情况、消费场所、社交、个人兴趣等丰富的多维度数据，参赛者通过分析建模，运用机器学习和深度学习算法，准确评估用户消费信用分值。

方案分享：

1、https://mp.weixin.qq.com/s/t0oIP6XPWeSxDV2_lsliiA

2、https://github.com/C-rawler/DCIC-2019-Credit-intelligence-score-2th-Place

3、https://github.com/xy0210/DCIC-2019-China-Mobile


【超大规模推荐之用户兴趣高效检索】

赛事方向：结构化数据

赛事简介：参赛选手需要为测试集中的每一个用户生成一个商品推荐列表，列表中需要包含该用户最有可能感兴趣的 50 个商品。选手提交的推荐结果将用于和真实的用户兴趣进行比对，推荐结果的精准度和新颖性将作为最终的评价指标并反馈给参赛者。

方案分享：

第一名：https://tianchi.aliyun.com/notebook-ai/detail?spm=5176.12586969.1002.12.762d5059KmffL5&postId=81152

第三名：https://tianchi.aliyun.com/notebook-ai/detail?spm=5176.12586969.1002.3.762d5059KmffL5&postId=81487

第四名：https://tianchi.aliyun.com/forum/postDetail?spm=5176.12586969.1002.9.762d5059KmffL5&postId=81500



赛事方向：结构化数据

赛事简介：购买转化率是品牌商家在电商平台运营时最关注的指标之一，本次大赛中云积互动提供了品牌商家的历史订单数据，参赛选手通过人工智能技术构建预测模型，预估用户人群在规定时间内产生购买行为的概率。

方案分享：

baseline：https://github.com/Travisgogogo/2019-datacastle-enbrands



赛事方向：结构化数据

赛事简介：从给定的房屋基本信息以及房屋销售信息等，建立一个回归模型预测房屋的销售价格。

方案分享：

baseline：https://github.com/bingshen/KingCounty



```

### NLP竞赛
```

赛事方向：自然语言处理

赛事简介：为应对当前虚假新闻泛滥的现状，将虚假新闻带来的危害最小化，我们设立此赛题以促进对虚假新闻自动化检测方法的研究。针对虚假新闻的特点，我们设立了三个子任务：虚假新闻文本检测、虚假新闻图片检测、虚假新闻多模态检测。

方案分享：

第一名：https://www.biendata.com/models/category/3529/L_notebook/

其他:https://github.com/deping-1/2019-false-news-detection-challenge


赛事方向：自然语言处理

赛事简介：本次比赛将提供一个论文库（约含20万篇论文），同时提供对论文的描述段落，来自论文中对同类研究的介绍。参赛选手需要为描述段落匹配三篇最相关的论文。

方案分享：

冠军:https://zhuanlan.zhihu.com/p/88664963

亚军:https://zhuanlan.zhihu.com/p/88257675



```

### 疫情期间互联网虚假新闻检测
```
比赛：https://discussion.datafountain.cn/questions/2638
代码：https://github.com/parthpatwa/covid19-fake-news-detection/blob/main/ml_baseline.ipynb


补充知识
论文
Rumor Detection on Social Media with Bi-Directional Graph Convolutional Networks
Capturing the Style of Fake News
Weak Supervision for Fake News Detection via Reinforcement Learning
Proactive Discovery of Fake News Domains from Real-Time Social Media Feeds


参考代码
https://github.com/piotrmp/fakestyle
https://github.com/yaqingwang/WeFEND-AAAI20

虚假新闻检测
https://github.com/YuzheMao/Multimodal-Fake-News-Detection-during-COVID-19
https://github.com/shibing624/fake-news-detector
https://jiaxiangbu.github.io/rumor_detection_2019_ncov/
https://github.com/YuzheMao/Multimodal-Fake-News-Detection-during-COVID-19

biendata-智源&计算所-互联网虚假新闻检测挑战赛
https://github.com/datawhalechina/competition-baseline/blob/master/competition/biendata-%E6%99%BA%E6%BA%90%26%E8%AE%A1%E7%AE%97%E6%89%80-%E4%BA%92%E8%81%94%E7%BD%91%E8%99%9A%E5%81%87%E6%96%B0%E9%97%BB%E6%A3%80%E6%B5%8B%E6%8C%91%E6%88%98%E8%B5%9B/README.md

2019虚假新闻检测挑战赛冠军思路&方法揭秘
https://www.secrss.com/articles/15352

参考文章
万字长文带你解读『虚假新闻检测』最新进展：https://cloud.tencent.com/developer/article/1774448
CIKM 2021 | 假新闻有“两幅面孔”：整合模式和事实信息的虚假新闻检测（已开源）：https://zhuanlan.zhihu.com/p/414464291
如何实现网络虚假信息的智能识别：https://www.ccf.org.cn/Media_list/cncc/2022-11-02/775967.shtml
```

### 2023数学建模C题方案
```
题目：http://www.mcm.edu.cn/html_cn/node/c74d72127066f510a5723a94b5323a26.html
全套解决方案：https://aistudio.baidu.com/projectdetail/6805057

数学建模资料：https://github.com/zhanwen/MathModel


论文 + 代码：https://github.com/HuaYuXiao/Automated-pricing-and-replenishment-decisions-for-vegetable-products
计算品类关联代码：https://github.com/jxtse/MCM2023_C/blob/main/%E5%8D%95%E5%93%81Apriori.py


简单思路参考
盗火的想法 - 知乎
https://www.zhihu.com/pin/1684959372697100288
2023国赛C题-蔬菜定价与补货-探索性思路及初步实现 - 模型视角的文章 - 知乎
https://zhuanlan.zhihu.com/p/654953916
2023数学建模C题国赛高教社杯模型代码 - 数学玩客的文章 - 知乎
https://zhuanlan.zhihu.com/p/655188680


官方解题思路：https://mp.weixin.qq.com/s/zO7i2mi0we2n1BEOkpNkQA


附件1给出了某商超经销的6个蔬菜品类的商品信息;附件2和附件3分别给出了该商超2020年7月1日至2023年6月30日各商品的销售流水明细与批发价格的相关数据;附件4给出了各商品近期的损耗率数据。请根据附件和实际情况建立数学模型解决以下问题:
问题1蔬菜类商品不同品类或不同单品之间可能存在一定的关联关系，请分析蔬菜各品类及单品销售量的分布规律及相互关系。

问题2考虑商超以品类为单位做补货计划，请分析各蔬菜品类的销售总量与成本加成定价的关系，并给出各蔬菜品类未来一周(2023年7月1-7日)的日补货总量和定价策略，使得商超收益最大。

问题3因蔬菜类商品的销售空间有限，商超希望进一步制定单品的补货计划，要求可售单品总数控制在27-33个，且各单品订购量满足最小陈列量25千克的要求。根据2023年6月24-30日的可售品种，给出7月1日的单品补货量和定价策略，在尽量满足市场对各品类蔬菜商品需求的前提下，使得商超收益最大。

问题4为了更好地制定蔬菜商品的补货和定价决策，商超还需要采集哪些相关数据，这些数据对解决上述问题有何帮助，请给出你们的意见和理由。
附件16个蔬菜品类的商品信息附件2销售流水明细数据
附件3蔬菜类商品的批发价格附件4蔬菜类商品的近期损耗率
注(1)附件1中，部分单品名称包含的数字编号表示不同的供应来源。
(2)附件4中的损耗率反映了近期商品的损耗情况，通过近期盘点周期的数据计算得到。


第二题
简单arima 或者 机器学习模型搭建 成本-销量模型
用mini 和公式 搭建最优化解法

第三题
品类粒度切换单品类
主体解法不变
加约束条件

df_profit[col] = df_sale[col].astype(float)*df_price[col].astype(float) - df_sale[col].astype(float) *(1+selected_columns[selected_columns['单品净名称'] == col]['平均损耗率'].values[0].astype(float))*df_cost[col].astype(float)

a=float(df_total.loc[(1078+i),[S]])+(float(df_sale.loc[(1078+i),[S]])-2.5)*(float(df_cost.loc[(1078+i),[S]]))/(np.log(3.5))*(np.log(3.5-x))

画图工具
pip3 install seaborn



中文乱码
安装字体
apt-get install fonts-wqy-zenhei
清楚字体缓存
python3 -c "import matplotlib; print(matplotlib.get_cachedir())"
rm  /root/.cache/matplotlib/fontlist-v330.json 



!wget http://129.204.205.246/downloads/SimHei.ttf
!rm -r /home/aistudio/.cache/matplotlib
!mkdir -p ~/.fonts
!cp SimHei.ttf ~/.fonts/SimHei.ttf
!fc-cache -fv
```

### 交叉验证
```
【机器学习】Cross-Validation（交叉验证）详解：https://zhuanlan.zhihu.com/p/24825503
```

### OTTO 
```
kaggle推荐系统比赛汇总 （含金牌方案） - 鱼子酱的文章 - 知乎
https://zhuanlan.zhihu.com/p/651824600

1st place solution
https://www.kaggle.com/competitions/otto-recommender-system/discussion/384022 

3st code
https://github.com/TheoViel/kaggle_otto_rs/tree/master


开发环境-硬件条件
Features are computed per batch on a 32Gb V100 using RAPIDS. It's fast
https://rapids.ai/
https://drive.google.com/drive/search?q=owner:me+(type:application/vnd.google.colaboratory+%7C%7C+type:application/vnd.google.colab)




```

## 基础
### Transformer
```
如何最简单、通俗地理解Transformer？ - 鱼先生的回答 - 知乎
https://www.zhihu.com/question/445556653/answer/2882383919
Transformer 之逐层介绍 - 鱼先生的文章 - 知乎
https://zhuanlan.zhihu.com/p/604450283
Transformer 之多头注意力 - 鱼先生的文章 - 知乎
https://zhuanlan.zhihu.com/p/604452790
Transformer 之注意力计算原理 - 鱼先生的文章 - 知乎
https://zhuanlan.zhihu.com/p/604454823


【【精校珍藏版】大牛Andrej Karpathy的stanford深度学习课程：深入理解Transformer，从零打造最简版GPT】 https://www.bilibili.com/video/BV1Tm4y1b7UP/?share_source=copy_web&vd_source=8783d6f7758784f093c06edba717af3d


源码实现（有能力自己看）
http://nlp.seas.harvard.edu/annotated-transformer/
https://wmathor.com/index.php/archives/1455/
https://github.com/BoXiaolei/MyTransformer_pytorch
nlp模型算法实现：https://github.com/graykode/nlp-tutorial


源码讲解
https://wmathor.com/index.php/archives/1438/

```


#### 整体思路
1. Key
2. Query
3. Value
4. QKV计算公式，如何初始化，最终达到注意力的效果

#### LSTM RNN 对比
1. 没有循环神经网络的迭代操作，所以我们必须提供每个字的位置信息给 Transformer，这样它才能识别出语言中的顺序关系
2. 位置嵌入的概念，也就是 Positional Encoding，位置嵌入的维度为 [max_sequence_length, embedding_dimension], 位置嵌入的维度与词向量的维度是相同的，都是 embedding_dimension。max_sequence_length 属于超参数，指的是限定每个句子最长由多少个词构成
3. 

#### 注意力机制
1. Embedding Size
2. Query
3. number of Attention heads
4. batch_size
思路灵感
1. Neural machine translation by jointly learning to align and translate
2. Massive exploration of neural machine translation architectures


#### layer-normal
```
思路灵感
Layer normalization

```
#### position encoding
```
思路灵感
Convolutional sequence to sequence learning



```

#### position-wise feedward networks
1. 

#### 网络结构优化
思路灵感
1. Rethinking the inception architecture for computer vision
2.  Dropout: a simple way to prevent neural networks from overfitting
3.  Deep residual learning for image recognition

### Bert
```

```

### resnet
```

```



### ernie-m 网络结构
```
NetWork(
  (ernie): ErnieMModel(
    (embeddings): ErnieMEmbeddings(
      (word_embeddings): Embedding(250002, 768, sparse=False)
      (position_embeddings): Embedding(514, 768, sparse=False)
      (layer_norm): LayerNorm(normalized_shape=[768], epsilon=1e-05)
      (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)
    )
    (encoder): TransformerEncoder(
      (layers): LayerList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)
          )
          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)
          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)
          )
          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)
          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)
          )
          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)
          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)
          )
          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)
          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)
          )
          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)
          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)
          )
          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)
          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (6): TransformerEncoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)
          )
          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)
          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (7): TransformerEncoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)
          )
          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)
          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (8): TransformerEncoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)
          )
          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)
          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (9): TransformerEncoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)
          )
          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)
          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (10): TransformerEncoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)
          )
          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)
          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
        (11): TransformerEncoderLayer(
          (self_attn): MultiHeadAttention(
            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)
            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)
          )
          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)
          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)
          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)
          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)
          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)
          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)
          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)
        )
      )
    )
    (pooler): ErnieMPooler(
      (dense): Linear(in_features=768, out_features=768, dtype=float32)
      (activation): Tanh()
    )
  )
  (resnet): EncoderCNN(
    (resnet): Sequential(
      (0): Conv2D(3, 64, kernel_size=[7, 7], stride=[2, 2], padding=3, data_format=NCHW)
      (1): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)
      (2): ReLU()
      (3): MaxPool2D(kernel_size=3, stride=2, padding=1)
      (4): Sequential(
        (0): BottleneckBlock(
          (conv1): Conv2D(64, 64, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(64, 64, kernel_size=[3, 3], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(64, 256, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
          (downsample): Sequential(
            (0): Conv2D(64, 256, kernel_size=[1, 1], data_format=NCHW)
            (1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2D(256, 64, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(64, 64, kernel_size=[3, 3], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(64, 256, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
        )
        (2): BottleneckBlock(
          (conv1): Conv2D(256, 64, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(64, 64, kernel_size=[3, 3], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(64, 256, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
        )
      )
      (5): Sequential(
        (0): BottleneckBlock(
          (conv1): Conv2D(256, 128, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(128, 128, kernel_size=[3, 3], stride=[2, 2], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(128, 512, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
          (downsample): Sequential(
            (0): Conv2D(256, 512, kernel_size=[1, 1], stride=[2, 2], data_format=NCHW)
            (1): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2D(512, 128, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(128, 512, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
        )
        (2): BottleneckBlock(
          (conv1): Conv2D(512, 128, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(128, 512, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
        )
        (3): BottleneckBlock(
          (conv1): Conv2D(512, 128, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(128, 512, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
        )
      )
      (6): Sequential(
        (0): BottleneckBlock(
          (conv1): Conv2D(512, 256, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(256, 256, kernel_size=[3, 3], stride=[2, 2], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
          (downsample): Sequential(
            (0): Conv2D(512, 1024, kernel_size=[1, 1], stride=[2, 2], data_format=NCHW)
            (1): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2D(1024, 256, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
        )
        (2): BottleneckBlock(
          (conv1): Conv2D(1024, 256, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
        )
        (3): BottleneckBlock(
          (conv1): Conv2D(1024, 256, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
        )
        (4): BottleneckBlock(
          (conv1): Conv2D(1024, 256, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
        )
        (5): BottleneckBlock(
          (conv1): Conv2D(1024, 256, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
        )
        (6): BottleneckBlock(
          (conv1): Conv2D(1024, 256, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
        )
        (7): BottleneckBlock(
          (conv1): Conv2D(1024, 256, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
        )
        (8): BottleneckBlock(
          (conv1): Conv2D(1024, 256, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
        )
        (9): BottleneckBlock(
          (conv1): Conv2D(1024, 256, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
        )
        (10): BottleneckBlock(
          (conv1): Conv2D(1024, 256, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
        )
        (11): BottleneckBlock(
          (conv1): Conv2D(1024, 256, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
        )
        (12): BottleneckBlock(
          (conv1): Conv2D(1024, 256, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
        )
        (13): BottleneckBlock(
          (conv1): Conv2D(1024, 256, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
        )
        (14): BottleneckBlock(
          (conv1): Conv2D(1024, 256, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
        )
        (15): BottleneckBlock(
          (conv1): Conv2D(1024, 256, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
        )
        (16): BottleneckBlock(
          (conv1): Conv2D(1024, 256, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
        )
        (17): BottleneckBlock(
          (conv1): Conv2D(1024, 256, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
        )
        (18): BottleneckBlock(
          (conv1): Conv2D(1024, 256, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
        )
        (19): BottleneckBlock(
          (conv1): Conv2D(1024, 256, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
        )
        (20): BottleneckBlock(
          (conv1): Conv2D(1024, 256, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
        )
        (21): BottleneckBlock(
          (conv1): Conv2D(1024, 256, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
        )
        (22): BottleneckBlock(
          (conv1): Conv2D(1024, 256, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
        )
      )
      (7): Sequential(
        (0): BottleneckBlock(
          (conv1): Conv2D(1024, 512, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(512, 512, kernel_size=[3, 3], stride=[2, 2], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(512, 2048, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=2048, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
          (downsample): Sequential(
            (0): Conv2D(1024, 2048, kernel_size=[1, 1], stride=[2, 2], data_format=NCHW)
            (1): BatchNorm2D(num_features=2048, momentum=0.9, epsilon=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2D(2048, 512, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(512, 512, kernel_size=[3, 3], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(512, 2048, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=2048, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
        )
        (2): BottleneckBlock(
          (conv1): Conv2D(2048, 512, kernel_size=[1, 1], data_format=NCHW)
          (bn1): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)
          (conv2): Conv2D(512, 512, kernel_size=[3, 3], padding=1, data_format=NCHW)
          (bn2): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)
          (conv3): Conv2D(512, 2048, kernel_size=[1, 1], data_format=NCHW)
          (bn3): BatchNorm2D(num_features=2048, momentum=0.9, epsilon=1e-05)
          (relu): ReLU()
        )
      )
    )
    (adaptive_pool): AdaptiveAvgPool2D(output_size=(1, 1))
  )
  (classifier1): Linear(in_features=5632, out_features=1024, dtype=float32)
  (classifier2): Linear(in_features=1024, out_features=3, dtype=float32)
  (attention_text): MultiHeadAttention(
    (q_proj): Linear(in_features=768, out_features=768, dtype=float32)
    (k_proj): Linear(in_features=768, out_features=768, dtype=float32)
    (v_proj): Linear(in_features=768, out_features=768, dtype=float32)
    (out_proj): Linear(in_features=768, out_features=768, dtype=float32)
  )
  (attention_image): MultiHeadAttention(
    (q_proj): Linear(in_features=2048, out_features=2048, dtype=float32)
    (k_proj): Linear(in_features=2048, out_features=2048, dtype=float32)
    (v_proj): Linear(in_features=2048, out_features=2048, dtype=float32)
    (out_proj): Linear(in_features=2048, out_features=2048, dtype=float32)
  )
)
```

### NLP 技术图谱

1. 自监督词表示学习
   1. 词向量模型(Word2Vec)
   2. 语言模型(Language Modeling, LM)
2. 句子编码网络
   1. 自回归 n-gram语言模型
   2. 循环神经网络(Recurrent Neural Network, RNN)
   3. 注意力机制 Transformer
3. 自回归、自编码预训练模型
   1. GPT(自回归)
      1. 无马尔科夫链
      2. self-attention
      3. 大规模自监督学习
   2. BERT 自编码
      1. cbow
      2. 利用上下文预测中间词
      3. 作为encoder 能看到整个句子的信号
4. 任务类型
   1. classification
   2. entailment
   3. similary
   4. multiple choice
5. 语言任务
   1. 字理解
   2. 词理解
   3. 句子理解
   4. 篇章理解
   5. 多义 同义 歧义
   6. 语境
   7. 语义理解
      1. 双向语言模型建模 建模上下文信息
      2. 两层lstm 建模不同层次语义信息 （单词特征 句法特征 语义特征）
   8. 单句分类
   9. 句对分类 自然语言推断
   10. 文本匹配
       1.  query - document
       2.  question - answer
       3.  utterance - response
   11. 命名实体识别
   12. 事件关系抽取
   13. 机器阅读理解
6. ELMo
   1. pre-training
      1. 双向语言模型建模 建模上下文信息
      2. 两层lstm 建模不同层次语义信息 （单词特征 句法特征 语义特征）
   2. fine-tuning
      1. 基于feature-based方式
   3. 问题
      1. 不完全双向训练
      2. 任务相关网络结构设计
      3. 仅有词向量，无句向量
7. GPT
   1. pre-training
      1. transform decoder
      2. bookscorpus
   2. fine-tuning
      1. 基于model-based方式
8. Bert
   1. pre-training
      1. LM -> Auto-encoder
      2. sentence-level
   2. fine-tuning
      1. add token-level
      2. add sentence-level
   3. 单句分类
   4. 句对分类
   5. 序列标注任务
9. ernie
   1.  ernie-vil
   2.  unimo
   3.  -M
   4.  -Doc
10. 语言
    1.  语序
    2.  语义：关联，非关联，同话题
    3.  逻辑关系
11. 信息抽取
    1.  实体
    2.  关系
    3.  事件
    4.  方法
        1.  抽取解析式
        2.  理解生成式
12. 问答系统
    1.  文本
    2.  知识库
    3.  表格
    4.  视频
    5.  方法
        1.  稀疏向量
        2.  稠密向量
13. 数学工具
   1. 前向传播
   2. 反向传播
   3. 损失函数
   4. 负梯度反向传播
14. 
15. 网络层
   1. embedding
   2. 

模型
1. 2013
   1. cbow skip-gram glove
2. 2014
   1. cnn rnn lstm seq-to-seq
3. 2015
   1. transformer
4. 2018
   1. elmo gpt bert

论文
1. 基础论文
   1. Efficient Estimation of Word Representations in Vector Space.
   2. RNN
   3. self-attention
   4. elmo
   5. gpt
   6. bert
   7. ernie
   8.  A continual pre-training framework for language understanding
2.  问答系统论文
    1.  Reading Wikipedia to Answer Open-domain Questions
    2.  Questions for Machine Comprehension of Text
    3.  DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world Applications
    4.  Dense Passage Retrieval for Open-Domain Question Answering.


### DSSM双塔模型
```
DSSM双塔模型及pytorch实现 - 简之的文章 - 知乎
https://zhuanlan.zhihu.com/p/402123341

https://github.com/MemoryForSky/deepctr

https://github.com/HeartFu/DSSM.git

```
 
### wide & deep & XDeepFM 
1. wide
   1. 特征构造
   2. 特征交叉
   3. FTRL优化器（特征稀疏）
2. depp
   1. embedding层
   2. nn层
   3. AdaGrad优化器

### FM & FFM & DeepFM & DeepFFM
```





```

### DCN & DCNv2

### bias问题
1. position-bias pCTR 华为
2. exposure-bias ESMM 阿里
3. popularity-bias ESAM

### 模型效果优化
1. 为什么回归问题不能用Dropout - Lukan的文章 - 知乎 https://zhuanlan.zhihu.com/p/561124500


### 精排模型
```
快手精排模型实践 - DataFunTalk的文章 - 知乎
https://zhuanlan.zhihu.com/p/602322538


一文说尽推荐系统中的精排模型 - Tang AI的文章 - 知乎
https://zhuanlan.zhihu.com/p/586162228

小红书如何实现高效推荐？解密背后的大数据计算平台架构 - 阿里云云栖号的文章 - 知乎
https://zhuanlan.zhihu.com/p/77409613

推荐算法—精排模型DIN/DIEN/SIM/DFN/DCIN - 一分钟666的文章 - 知乎
https://zhuanlan.zhihu.com/p/674125085


```



#### embedding 空间对齐
```

```

#### 特征重要性对齐
```

```



### 混排 粗排
```
WWW2022｜美团基于强化学习的信息流广告混排算法 - papacai的文章 - 知乎
https://zhuanlan.zhihu.com/p/558087806

阿里定向广告最新突破：面向下一代的粗排排序系统COLD - 萧瑟的文章 - 知乎
https://zhuanlan.zhihu.com/p/186320100

阿里广告技术最新突破：全链路联动-面向最终目标的全链路一致性建模 - 萧瑟的文章 - 知乎
https://zhuanlan.zhihu.com/p/413240790

工业界（搜索 推荐）粗排模型一般怎么做？ - 谢杨易的回答 - 知乎
https://www.zhihu.com/question/441037971/answer/3338246520

召回算法有哪些？ - 余文毅的回答 - 知乎
https://www.zhihu.com/question/423384620/answer/3344167214

OPPO 广告全链路一致性召回算法实践与探索 - 余文毅的文章 - 知乎
https://zhuanlan.zhihu.com/p/675351007


推荐系统多目标优化专题(1)——深入理解推荐系统 - iwtbs的文章 - 知乎
https://zhuanlan.zhihu.com/p/476753154

广告召回论文阅读笔记（2）-从TDM到二向箔 - magicwt的文章 - 知乎
https://zhuanlan.zhihu.com/p/675418752


阿里的TDM树深度模型为什么很少有人用，是有哪些问题吗？ - magicwt的回答 - 知乎
https://www.zhihu.com/question/485938484/answer/3349977619


OPPO 广告召回算法实践与探索 - DataFunTalk的文章 - 知乎
https://zhuanlan.zhihu.com/p/675609025

```

## 统计
```

字节跳动数据挖掘岗面经(杭州) - starfly的文章 - 知乎
https://zhuanlan.zhihu.com/p/518212490



```

### 基础概念
1. 标准误差SE 
2. 置信区间
3. p值：
1. 差值：实验组-对照组数量
2. 变化率：差值/对照组数量
3. T分布，Z分布
4. F检验，卡方检验

### 置信区间
```
如何通俗地解释「置信区间」和「置信水平」？ - 猴子的回答 - 知乎
https://www.zhihu.com/question/24801731/answer/251576717


相爱相杀的置信区间和p值 - 医小咖的文章 - 知乎
https://zhuanlan.zhihu.com/p/32432629


Z检验、T检验下 P-value 和置信区间的计算 - energy百分百的文章 - 知乎
https://zhuanlan.zhihu.com/p/386921569

火山引擎ab实验
https://www.volcengine.com/docs/6287/65818
https://www.volcengine.com/docs/6287/65837

【AB实验统计学】P-value和置信区间 - Effyyy的文章 - 知乎
https://zhuanlan.zhihu.com/p/372485716



```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```


## 课程
### 李宏毅ML课程 
```
https://speech.ee.ntu.edu.tw/~hylee/ml/2023-spring.php



```

### Andrej Karpathy课程
```
【【精校珍藏版】大牛Andrej Karpathy的stanford深度学习课程：深入理解Transformer，从零打造最简版GPT】 https://www.bilibili.com/video/BV1Tm4y1b7UP/?share_source=copy_web&vd_source=8783d6f7758784f093c06edba717af3d

大佬的思路
搞清 history of attention
自编码器
隐藏状态加权和


```

### 数据库2021~2023回顾
```
万字带你走过数据库的这激荡的三年 - NebulaGraph的文章 - 知乎
https://zhuanlan.zhihu.com/p/684728712

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```

### 
```

```
## 环境

bml
1. 限制paddle
2. 第三方包下载总是有问题
魔塔
1. 限制魔塔包
macbook
1. gpu有限制
ubuntu20
1. 版本均OK
ubuntu18
1. git版本低
2. python低

并行计算
1. 多线程
2. 多进程




## 从0到1安装
```bash
ubuntu
apt-get update
apt install sudo -y
# programm
apt install -y gcc-9 g++-9
# golang 1.20版本
wget https://mirrors.ustc.edu.cn/golang/go1.21.0.linux-amd64.tar.gz
tar -zxvf go1.21.0.linux-amd64.tar.gz
rm -rf /usr/local/go && tar -C /usr/local -xzf go1.21.0.linux-amd64.tar.gz
rm -rf /usr/bin/go && tar -C /usr/bin -xzf go1.21.0.linux-amd64.tar.gz
export PATH=$PATH:/usr/local/go/bin
go version
go env -w GO111MODULE=on
go env -w GOPROXY=https://goproxy.cn,direct



# pg库
apt-get install libpq-dev -y
apt install postgresql -y

# 创建库表 - 业务操作
# service postgresql start
# sudo -u postgres psql
# CREATE USER dbusername WITH PASSWORD 'dbpassword';
# CREATE DATABASE db;


# python3
apt install -y python3.8
apt install -y python3-pip
# openGL库
apt-get install -y libgl1-mesa-glx
# 进度条库
pip3 install tqdm   
# sql
pip3 install SQLAlchemy==1.4.23 -i https://pypi.mirrors.ustc.edu.cn/simple
pip3 install psycopg2 


# pytorch
pip3 install torch torchvision -i https://pypi.mirrors.ustc.edu.cn/simple # 用国内源
pip3 install graphviz -i https://pypi.mirrors.ustc.edu.cn/simple

# huggingface
pip3 install setuptools_rust
pip3 install transformers

# juypter
pip3 install setuptools_scm
pip3 install argon2-cffi-bindings
pip3 install jupyter



# paddlepaddle 全家桶
pip3 install paddlepaddle==2.5.1 -i https://pypi.tuna.tsinghua.edu.cn/simple
pip3 install paddleocr
pip3 install visualdl==2.4.0  
pip3 install flask-babel==1.0.0
pip3 install paddlenlp -i https://pypi.tuna.tsinghua.edu.cn/simple    

# pdf工具
pip3 install pdf2image
pip3 install gradio
pip3 install pdfplumber   

pip3 install langchain
# embedding
pip3 install fastNLP  
pip3 install fuzzywuzzy
python3 -m pip install milvus
pip3 install jieba

# modelscope
pip3 install numpy pandas urllib3
pip3 install datasets==2.13.0
pip3 install dill==0.3.6
pip3 install multiprocess==0.70.14
pip3 install accelerate -U
pip3 install sentencepiece -U
pip3 install modelscope -U 

# 打包当前环境
pip3 freeze > requirements.txt



# 业务逻辑
# llm_demo服务
dir=`pwd`
export PYTHONPATH=$PYTHONPATH:$dir
export PYTHONPATH=$PYTHONPATH:/mnt/workspace

# 写入数据
cd /mnt/workspace/llm_demo/util
python3 insert_balance_sheet.py
python3 insert_cash_flow.py
python3 insert_company_annual_reports.py
python3 insert_profit_statement.py

mv /etc/dsw/lib/libffi.so.8.1.0 /mnt/workspace/libffi/.
mv /etc/dsw/lib/libffi.so.8 /mnt/workspace/libffi/.
mv /opt/conda/lib/libffi.so.8 /mnt/workspace/libffi/.
mv /opt/conda/lib/libffi.so.8.1.0 /mnt/workspace/libffi/.
mv /opt/conda/pkgs/libffi-3.4.2-h6a678d5_6/lib/libffi.so.8.1.0 /mnt/workspace/libffi/.
mv /opt/conda/pkgs/libffi-3.4.2-h6a678d5_6/lib/libffi.so.8 /mnt/workspace/libffi/.


cp /mnt/workspace/libffi/libffi.so.8 /opt/conda/lib/libffi.so.8 



```

## 包编译
### 全局配置
cur_dir=/docker/root/projects/demo/package
export PATH="$cur_dir/cmake-3.24.0-linux-x86_64/bin:$PATH"
export PATH="$cur_dir/lib:$cur_dir/bin:$PATH"


### cmake
wget https://cmake.org/files/v3.24/cmake-3.24.0-linux-x86_64.tar.gz
tar xf cmake-3.24.0-linux-x86_64.tar.gz
export PATH=$cur_dir/include:$cur_dir/bin:$cur_dir/cmake-3.24.0-linux-x86_64/bin:/root/.nvm/versions/node/v14.17.2/bin:/root/.nvm/versions/node/v14.17.2/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
export LD_LIBRARY_PATH=/usr/local/lib
export LIBRARY_PATH=/usr/local/lib:$cur_dir/lib:$cur_dir/include
export LAPACK_LIBRARIES=/usr/local/lib

### 基础库
```
# gfortran
apt-get install gfortran -y

# libsndfile
apt-get install libsndfile1 -y

# OpenBLAS
apt-get install build-essential -y
git clone https://github.com/xianyi/OpenBLAS.git
cd OpenBLAS
# x86_64
make TARGET=NEHALEM
make
make PREFIX=$cur_dir install


# LAPACK
# 参考 https://blog.csdn.net/dante0610/article/details/113853805
apt-get install libscalapack-mpi-dev -y
apt-get install liblapack-dev -y

wget http://www.netlib.org/lapack/lapack-3.4.2.tgz
tar -xvf lapack-3.4.2.tgz
cd lapack-3.4.2
mkdir build
cd build
cmake ..
make -j4
make PREFIX=$cur_dir install

# gTest
git clone --depth 1 https://github.com/google/googletest.git
cd googletest
mkdir build && cd build
cmake ..
make -j40
make PREFIX=$cur_dir install

```

### faiss
/docker/root/projects/demo/package/lib/libopenblas.a

cmake -B build -DLAPACK_LIBRARIES=/usr/local/lib/liblapack.a -DBLAS_LIBRARIES=$cur_dir/lib/libopenblas.a  -DFAISS_ENABLE_GPU=OFF .
make -C build clean && make -C build -j40 && make -C build PREFIX=$cur_dir install

cmake -B build -DLAPACK_LIBRARIES=/usr/lib/x86_64-linux-gnu/lapack -DBLAS_LIBRARIES=$cur_dir/lib  -DFAISS_ENABLE_GPU=OFF -DBUILD_SHARED_LIBS=ON .


cd build
make -j40
make PREFIX=$cur_dir install

### puck


### starrocks


### pytorch
```bash


# 安装conda
wget https://repo.anaconda.com/archive/Anaconda3-2023.09-0-Linux-x86_64.sh

# 自动化安装
sh Anaconda3-2023.09-0-Linux-x86_64.sh -b -u


# cmake安装
# 略

alias python=python3
alias gcc=/usr/bin/gcc
alias g++=/usr/bin/g++
# gcc 路径会在pytorch中被用到
mv /usr/bin/gcc-9 /usr/bin/gcc
mv /usr/bin/g++-9 /usr/bin/g++
export PATH="/root/anaconda3/bin:$PATH"

# cmake 配置 指定编译器
export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-"$(dirname $(which conda))/../"}
export CMAKE_C_COMPILER=/usr/bin/gcc
export CMAKE_CXX_COMPILER=/usr/bin/g++


# Ninja 生成代码需要指定路径 手动修改
# export CMAKE_BUILD_WITH_INSTALL_RPATH=ON
# set(CMAKE_BUILD_WITH_INSTALL_RPATH TRUE)


# 安装第三方依赖，只能用pip
# conda install --file requirements.txt
conda install -c https://mirrors.aliyun.com/anaconda/ cmake ninja -y
conda install cmake ninja -y
pip install -i https://mirrors.aliyun.com/pypi/simple pyqt5==5.15
pip install -i https://mirrors.aliyun.com/pypi/simple pyqtwebengine==5.15
pip install -i https://mirrors.aliyun.com/pypi/simple -r requirements.txt

# 国内镜像源
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
conda config --set show_channel_urls yes


conda install -c https://mirrors.aliyun.com/anaconda/ intel::mkl-static intel::mkl-include -y
# CUDA only: Add LAPACK support for the GPU if needed
conda install -c pytorch magma-cuda110  # or the magma-cuda* that matches your CUDA version from https://anaconda.org/pytorch/repo

# (optional) If using torch.compile with inductor/triton, install the matching version of triton
# Run from the pytorch directory after cloning
make triton

git clone --recursive https://github.com/pytorch/pytorch
cd pytorch
git submodule sync
git submodule update --init --recursive

## https://gitclone.com/github.com/xxx 可以加速 git submodule
# 参考
# https://github.com/NVlabs/cub.git 改成下面
# https://gitclone.com/github.com/NVlabs/cub.git

# 国内访问限制问题，需要手动clone
cd third_party

rm -rf ios-cmake
git clone https://521github.com/Yangqing/ios-cmake.git

rm -rf psimd
git clone https://521github.com/Maratyszcza/psimd.git


rm -rf QNNPACK
git clone https://521github.com/pytorch/QNNPACK

rm -rf foxi
git clone https://521github.com/houseroad/foxi.git
cd ..


export MAX_JOBS=4
python setup.py develop > build.log



```

### pytorch配置
```
https://mirror.tuna.tsinghua.edu.cn/help/anaconda/

先执行 conda config --set show_channel_urls yes 生成该文件之后再修改

channels:
  - defaults
show_channel_urls: true
default_channels:
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2
custom_channels:
  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  deepmodeling: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/


```

### pytorch submodule修改
```
[submodule "third_party/pybind11"]
    ignore = dirty
    path = third_party/pybind11
    url = https://gitclone.com/github.com/pybind/pybind11.git
[submodule "third_party/cub"]
    ignore = dirty
    path = third_party/cub
    url = https://gitclone.com/github.com/NVlabs/cub.git
[submodule "third_party/eigen"]
    ignore = dirty
    path = third_party/eigen
    url = https://gitlab.com/libeigen/eigen.git
[submodule "third_party/googletest"]
    ignore = dirty
    path = third_party/googletest
    url = https://gitclone.com/github.com/google/googletest.git
[submodule "third_party/benchmark"]
    ignore = dirty
    path = third_party/benchmark
    url = https://gitclone.com/github.com/google/benchmark.git
[submodule "third_party/protobuf"]
    ignore = dirty
    path = third_party/protobuf
    url = https://gitclone.com/github.com/protocolbuffers/protobuf.git
[submodule "third_party/ios-cmake"]
    ignore = dirty
    path = third_party/ios-cmake
    url = https://gitclone.com/github.com/Yangqing/ios-cmake.git
[submodule "third_party/NNPACK"]
    ignore = dirty
    path = third_party/NNPACK
    url = https://gitclone.com/github.com/Maratyszcza/NNPACK.git
[submodule "third_party/gloo"]
    ignore = dirty
    path = third_party/gloo
    url = https://gitclone.com/github.com/facebookincubator/gloo
[submodule "third_party/NNPACK_deps/pthreadpool"]
    ignore = dirty
    path = third_party/pthreadpool
    url = https://gitclone.com/github.com/Maratyszcza/pthreadpool.git
[submodule "third_party/NNPACK_deps/FXdiv"]
    ignore = dirty
    path = third_party/FXdiv
    url = https://gitclone.com/github.com/Maratyszcza/FXdiv.git
[submodule "third_party/NNPACK_deps/FP16"]
    ignore = dirty
    path = third_party/FP16
    url = https://gitclone.com/github.com/Maratyszcza/FP16.git
[submodule "third_party/NNPACK_deps/psimd"]
    ignore = dirty
    path = third_party/psimd
    url = https://gitclone.com/github.com/Maratyszcza/psimd.git
[submodule "third_party/zstd"]
    ignore = dirty
    path = third_party/zstd
    url = https://gitclone.com/github.com/facebook/zstd.git
[submodule "third_party/cpuinfo"]
    ignore = dirty
    path = third_party/cpuinfo
    url = https://gitclone.com/github.com/pytorch/cpuinfo.git
[submodule "third_party/python-peachpy"]
    ignore = dirty
    path = third_party/python-peachpy
    url = https://gitclone.com/github.com/malfet/PeachPy.git
[submodule "third_party/onnx"]
    ignore = dirty
    path = third_party/onnx
    url = https://gitclone.com/github.com/onnx/onnx.git
[submodule "third_party/onnx-tensorrt"]
    ignore = dirty
    path = third_party/onnx-tensorrt
    url = https://gitclone.com/github.com/onnx/onnx-tensorrt
[submodule "third_party/sleef"]
    ignore = dirty
    path = third_party/sleef
    url = https://gitclone.com/github.com/shibatch/sleef
[submodule "third_party/ideep"]
    ignore = dirty
    path = third_party/ideep
    url = https://gitclone.com/github.com/intel/ideep
[submodule "third_party/nccl/nccl"]
    ignore = dirty
    path = third_party/nccl/nccl
    url = https://gitclone.com/github.com/NVIDIA/nccl
[submodule "third_party/gemmlowp/gemmlowp"]
    ignore = dirty
    path = third_party/gemmlowp/gemmlowp
    url = https://gitclone.com/github.com/google/gemmlowp.git
[submodule "third_party/QNNPACK"]
    ignore = dirty
    path = third_party/QNNPACK
    url = https://gitclone.com/github.com/pytorch/QNNPACK
[submodule "third_party/neon2sse"]
    ignore = dirty
    path = third_party/neon2sse
    url = https://gitclone.com/github.com/intel/ARM_NEON_2_x86_SSE.git
[submodule "third_party/fbgemm"]
    ignore = dirty
    path = third_party/fbgemm
    url = https://gitclone.com/github.com/pytorch/fbgemm
[submodule "third_party/foxi"]
    ignore = dirty
    path = third_party/foxi
    url = https://gitclone.com/github.com/houseroad/foxi.git
[submodule "third_party/tbb"]
    path = third_party/tbb
    url = https://gitclone.com/github.com/01org/tbb
    branch = tbb_2018
[submodule "android/libs/fbjni"]
    ignore = dirty
    path = android/libs/fbjni
    url = https://gitclone.com/github.com/facebookincubator/fbjni.git
[submodule "third_party/XNNPACK"]
    ignore = dirty
    path = third_party/XNNPACK
    url = https://gitclone.com/github.com/google/XNNPACK.git
[submodule "third_party/fmt"]
    ignore = dirty
    path = third_party/fmt
    url = https://gitclone.com/github.com/fmtlib/fmt.git
[submodule "third_party/tensorpipe"]
    ignore = dirty
    path = third_party/tensorpipe
    url = https://gitclone.com/github.com/pytorch/tensorpipe.git
[submodule "third_party/cudnn_frontend"]
	path = third_party/cudnn_frontend
	url = https://gitclone.com/github.com/NVIDIA/cudnn-frontend.git
[submodule "third_party/kineto"]
    path = third_party/kineto
    url = https://gitclone.com/github.com/pytorch/kineto
[submodule "third_party/pocketfft"]
	path = third_party/pocketfft
	url = https://gitclone.com/github.com/mreineck/pocketfft
[submodule "third_party/ittapi"]
	path = third_party/ittapi
	url = https://gitclone.com/github.com/intel/ittapi.git
[submodule "third_party/flatbuffers"]
	path = third_party/flatbuffers
	url = https://gitclone.com/github.com/google/flatbuffers.git
[submodule "third_party/nlohmann"]
	path = third_party/nlohmann
	url = https://gitclone.com/github.com/nlohmann/json.git
[submodule "third_party/VulkanMemoryAllocator"]
	path = third_party/VulkanMemoryAllocator
	url = https://gitclone.com/github.com/GPUOpen-LibrariesAndSDKs/VulkanMemoryAllocator.git
[submodule "third_party/cutlass"]
	path = third_party/cutlass
	url = https://gitclone.com/github.com/NVIDIA/cutlass.git
[submodule "third_party/mimalloc"]
	path = third_party/mimalloc
	url = https://gitclone.com/github.com/microsoft/mimalloc.git


```


### paddle
