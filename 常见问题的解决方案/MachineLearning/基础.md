## 机器学习基础

## 目录参考
1. 理解时间
2. 背景介绍
3. 参考资料
4. 核心思考问题
5. 入门概念
6. 阅读笔记
7. 项目工作流
8. 技术流图和图解
9.  源码目录
10. 模块拆解-横向
11. 模块拆解-纵向
12. 性能总结
13. 设计总结
14. 经验总结
15. 第三方依赖
16. 应用场景
17. 业务通点
18. 行业实践
19. case代码


### 理解时间
```
2024年10月10号启动

永远带着问题/需求/目标/兴趣/收益看代码

源码理解角度
   高层次流图分析 - 更好把握主次
    比如大数据框架 考虑流式计算范式
    比如机器学习框架 考虑数学计算流图
    共性分析数据格式 存储 读写 和 网络流图
    业务使用流程图和场景图
   横向梳理所有模块
   纵向梳理某个功能点
   编译角度
   使用角度
   性能角度
   底层数据结构角度

完整理解机器学习基础项目
如果只是在搜索引擎 搜 机器学习基础是远远不够的
机器学习基础 + 架构图

机器学习基础 + 概念关键词

机器学习基础 + 问题排查

机器学习基础 + 面试汇总

机器学习基础 + 极客挑战赛

机器学习基础 + 论坛会议

机器学习基础 + 论文

机器学习基础 + 前沿分享

机器学习基础 + 场景应用

机器学习基础 + 机器学习基础大佬名字

机器学习基础 + 公司项目
等等才能完全熟悉机器学习基础


```



## 大框
### blog
1. 袁进辉：https://www.zhihu.com/people/yuan-dong-39
2. https://www.zhihu.com/people/qyjdef
3. https://www.zhihu.com/people/66ring
4. 王荣生：https://github.com/WangRongsheng

### 深度思考
1. 算法
   1. 分类和回归到底什么区别
   2. 万能近似定理 是什么
   3. 主流网络层：全连接层（Fully-connected layer）、卷积层（Convolution layer）、循环网络层（Recurrent neral network layer）和注意力层（Attention layer）
   4. 评价指标
   5. 损失函数
   6. 
2. 工程
   1. 数据流图（Dataflow Graph）和 张量核心设计与结合
      1. 数据流图优化，运行时调度策略，以及算子优化
   2. 单设备算子间调度
   3. 图切分与多设备执行
   4. 动态图向静态图转换分为基于追踪（Tracing）和基于源代码解析（Parsing）两种方式
   5. 以TensorFlow的Auto-graph[6]和PyTorch的JIT[8]为代表，主流深度学习框架最终都走向了探索动态图与静态图的融合
   6. Dynamic Control Flow in Large-Scale Machine Learning
   7. 在CPU上实现一个矩阵乘法算子
      1. https://github.com/microsoft/AI-System/blob/main/Labs/BasicLabs/Lab2/mnist_custom_linear.py
      2. https://github.com/microsoft/AI-System/blob/main/Labs/BasicLabs/Lab2/mnist_custom_linear_cpp.py
   8. GPU
      1. https://github.com/microsoft/AI-System/blob/main/Labs/BasicLabs/Lab3/mnist_custom_linear_cuda.py
   9.  计算访存比是在针对硬件特点优化算法的常用指标，请参考文本中针对矩阵乘法的计算方法，给其它常用的算子计算其计算访存比和分块大小的关系，并推测其在GPU和CPU上实现的时候最关键的性能因素。
   10. 计算图上做的这些优化和传统编译器上的优化有何不同？你还能想到哪些计算图上的优化方法？
   11. 请读者思考为何进行编译时调度而不是运行时调度？如何实现运行时调度最可行的方案是什么？
   12. 内存和计算的优化之间是否互相影响？还有哪些有损的内存优化方法可以用在深度学习计算中？
   13. 在传统的编译器程序生成中，我们很少看到利用机器学习来自动生成程序的方法，请读者思考这种方法的好处与主要缺点，还有自动代码生成还能被用到哪些场景中呢？
   14. 为什么模型训练通常需要分布式进行，而分布式模型预测并不常见？
       1.  计算模式不同：预测任务占用存储更小，更容易放在单个设备中
       2.  训练需要各个工作节点（Worker）保持通信，从而协调统一地更新模型参数；
       3.  预测中的模型参数是固定的，各个工作节点分别使用只读副本，无需相互通信协调
   15. AllReduce的实现和优化
       1.  https://github.com/microsoft/AI-System/blob/main/Textbook/%E7%AC%AC6%E7%AB%A0-%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E7%AE%97%E6%B3%95%E4%B8%8E%E7%B3%BB%E7%BB%9F/6.5-%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E7%9A%84%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%B0%83.md
   16. 利用C++/CUDA API实现更为高效的压缩/解压缩编码
   17. 调度问题
       1.  如何提交作业与解决环境依赖问题？
       2.  如何高效调度作业并分配资源？
       3.  如何将启动的作业运行时环境，资源与命名空间隔离？
       4.  如何面向深度学习作业和异构资源设计集群管理系统？
       5.  如何高效存取数据？
       6.  如何不断开发平台新功能与运维平台并保证稳定性？
   18. 总结思考深度学习作业和传统操作系统作业以及大数据平台作业的异同点？
   19. 如果需要兼顾以上指标，新一代深度学习调度与平台设计应该朝着哪个方向设计与发展？
   20. 请读者设计算法或策略，保证公平性的同时，最大限度提升集群效率，可以上一小节的日志痕迹（Trace）进行实验设计与算法验证。
   21. 请读者思考，当前是否有必要设计一款针对深度学习场景的文件系统？



### 架构
1. 深度神经网络编译器
   1. DSL python go c++
   2. 中间表达（Intermediate Representation, IR）
   3. 算子表达式（Tensor Expression）
   4. 深度神经网络编译器的优化过程（Optimization Pass）
      1. 如常数传播、公共子表达式消除等
   5. 计算图优化
      2. 算术表达式化简
      3. 公共子表达式消除（Common Subexpression Elimination, CSE）
      4. 常数传播（constant propagation）就叫常数折叠（constant folding）
      5. 矩阵乘自动融合 BatchMatMul
      6. 算子融合方法是针对矩阵乘算子
         1. 针对大量的小算子的融合都可以提高GPU的利用率，减少内核启动开销、减少访存开销等好处。例如，Element-wise的算子（如Add，Mul，Sigmoid，Relu等）其计算量非常小，主要计算瓶颈都在内存的读取和写出上，如果前后的算子能够融合起来，前面算子的计算结果就可以直接被后面算子在寄存器中使用，避免数据在内存的读写，从而提交整体计算效率。
      7. 子图替换和随机子图替换
         1. 编译器在计算图中识别出一个子图并替换成一个等价的新的算子或子图的过程就是子图替换优化 
   6. 内存优化
      1. 基于拓扑序的最小内存分配
      2. 张量重计算
      3. 张量换入换出
   7. 内核优化
      1. 算子表达式
      2. 算子表示与调度逻辑的分离
      3. 自动调度搜索与代码生成
   8. 跨算子的全局调度优化
      1. 任意算子的融合
      2. 编译时全局算子调度
      3. 单个Op的调度时间与计算时间相比不可忽略，造成较大的调度开销；
      4. OP的并行度不足以占满GPU的计算核心。
2. 算子
   1. Add Log MatMul Conv BatchNorm Loss
3. 并行计算
   1. 串行计算到并行计算的演进
      1. 从原理上来讲，并行计算通过并行算法将问题的求解进行划分，并将编译的指令分发给分布式系统中的多处理器并行执行。这样一来，所有的计算量被分摊到多个计算单元之上，相比于串行计算，并行计算中的每个计算单元只需要负责部分的计算，缩短了整体的计算时间。
   2. 并行计算加速定律
      1. 阿姆达尔定律 (Amdahl's law)
      2. Gustafson定律 (Gustafson’s law)
   3. 深度学习的并行化训练
      1. 单步计算量取决于模型的复杂程度和批尺寸（Batch Size），结合计算速率可以算得训练耗时
      2. 并行训练BERT[7] 引入了全新的LARS优化器才能保证模型在设置了更大的批尺寸后依然能够收敛
   4. 算子内并行保持已有的算子的组织方式，探索将单个深度学习算子有效地映射到并行硬件设备上的执行
   5. 算子间并行则更注重发掘多个算子在多个设备上并行执行的策略，甚至解耦已有的单个算子为多个等效算子的组合，进一步发掘并行性。
4. 分布式训练
   1. 数据并行
   2. 模型并行
   3. 流水并行
   4. 同步并行、异步并行、半同步并行
   5. 机器内通信
      1. 共享内存、GPUDirect P2P over PCIe、GPUDirect P2P over NVLink
   6. 机器间通信
      1. TCP/IP网络、 RDMA网络和GPUDirect RDMA网络
5. 通信库
   1. NCCL: NVIDIA Collective Communication Library
   2. 集合式通信（collective communication）all-gather、 all-reduce、 broadcast、 reduce、reduce-scatter 以及点对点(point-to-point)通信send 和receive。
6. 调度问题优化目标
   1.  


## 算法 与 工程关系



## AB实验
1. 什么是ab实验
2. 什么是人群打散
3. 什么是算法人群包
4. 如何定义好一个可信的实验
5. 如何快速验证实验效果


## 课程
### 李宏毅ML课程 
```
https://speech.ee.ntu.edu.tw/~hylee/ml/2023-spring.php



```

### Andrej Karpathy课程
```
【【精校珍藏版】大牛Andrej Karpathy的stanford深度学习课程：深入理解Transformer，从零打造最简版GPT】 https://www.bilibili.com/video/BV1Tm4y1b7UP/?share_source=copy_web&vd_source=8783d6f7758784f093c06edba717af3d

大佬的思路
搞清 history of attention
自编码器
隐藏状态加权和


```

### 数据库2021~2023回顾
```
万字带你走过数据库的这激荡的三年 - NebulaGraph的文章 - 知乎
https://zhuanlan.zhihu.com/p/684728712

```