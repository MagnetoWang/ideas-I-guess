## 机器学习工程

## 目录参考
1. 理解时间
2. 背景介绍
3. 参考资料
4. 核心思考问题
5. 入门概念
6. 阅读笔记
7. 项目工作流
8. 技术流图和图解
9.  源码目录
10. 模块拆解-横向
11. 模块拆解-纵向
12. 性能总结
13. 设计总结
14. 经验总结
15. 第三方依赖
16. 应用场景
17. 业务通点
18. 行业实践
19. case代码


### 理解时间
```
2024年10月10号启动

永远带着问题/需求/目标/兴趣/收益看代码

源码理解角度
   高层次流图分析 - 更好把握主次
    比如大数据框架 考虑流式计算范式
    比如机器学习框架 考虑数学计算流图
    共性分析数据格式 存储 读写 和 网络流图
    业务使用流程图和场景图
   横向梳理所有模块
   纵向梳理某个功能点
   编译角度
   使用角度
   性能角度
   底层数据结构角度

完整理解机器学习工程项目
如果只是在搜索引擎 搜 机器学习工程是远远不够的
机器学习工程 + 架构图

机器学习工程 + 概念关键词

机器学习工程 + 问题排查

机器学习工程 + 面试汇总

机器学习工程 + 极客挑战赛

机器学习工程 + 论坛会议

机器学习工程 + 论文

机器学习工程 + 前沿分享

机器学习工程 + 场景应用

机器学习工程 + 机器学习工程大佬名字

机器学习工程 + 公司项目
等等才能完全熟悉机器学习工程





```

## 大框
1. 关注工程原理，而不是各类框架资料收集
2. AI 调度器
   1. Gang-Schedule：https://xigang.github.io/2019/02/17/gang-scheduler/
   2. AllReduce
      1. 【深度学习】【分布式训练】DeepSpeed：AllReduce与ZeRO-DP - 白强伟的文章 - 知乎https://zhuanlan.zhihu.com/p/610587671
      2. 关于AllReduce - Yun Zhou的文章 - 知乎 https://zhuanlan.zhihu.com/p/100012827
      3. 腾讯机智团队分享--AllReduce算法的前世今生 - 兰瑞Frank的文章 - 知乎 https://zhuanlan.zhihu.com/p/79030485
      4. ring allreduce和tree allreduce的具体区别是什么？ - Garvin Li的回答 - 知乎 https://www.zhihu.com/question/57799212/answer/292494636
      5. 深度学习常见AllReduce算法图解 - 李豪的文章 - 知乎 https://zhuanlan.zhihu.com/p/469942194
3. 显存优化
   1. DeepSpeed之ZeRO系列：将显存优化进行到底 - basicv8vc的文章 - 知乎https://zhuanlan.zhihu.com/p/513571706
4. 多卡多机训练
   1. pytorch分布式多机多卡训练，希望从例子解释，以下代码中参数是什么意思？ - 骑着白马的王子的回答 - 知乎 https://www.zhihu.com/question/453920336/answer/2289510851
   2. pytorch分布式多机多卡训练，希望从例子解释，以下代码中参数是什么意思？ - 祖新星的回答 - 知乎 https://www.zhihu.com/question/453920336/answer/3287739339
5. 特征
   1. 
6. 样本
   1. How To Create TFRecords：https://www.kaggle.com/code/alexxanderlarko/how-to-create-tfrecords
7. 模型
   1. 
8. 中间表达式 IR
   1. 如何自学AI编译器开发？ - 算树平均数的回答 - 知乎 https://www.zhihu.com/question/564620976/answer/3353966197
9. 推理
   1. 


## 深度思考
1. 交叉验证能否处理时序数据
   1. 无法处理时序数据！！！

## 资料
1. 机器学习编译：https://github.com/doongz/mlc-ai
2. 机器学习：https://github.com/stas00/ml-engineering
   1. 【ml-engineering 翻译系列】AI系统中的网络合集 - BBuf的文章 - 知乎 https://zhuanlan.zhihu.com/p/5407627544

## 阅读笔记

### 分布式机器学习——系统、工程与实战 柳浩


### 罗西blog
1. TensorFlow 分布式环境(1) :https://www.cnblogs.com/rossiXYZ/p/16014121.html

## 环境
1. 需要一个自动跳转的docker开发环境 支持cpp python go rust等一站式超级开发环境

bml
1. 限制paddle
2. 第三方包下载总是有问题
魔塔
1. 限制魔塔包
macbook
1. gpu有限制
ubuntu20
1. 版本均OK
ubuntu18
1. git版本低
2. python低

并行计算
1. 多线程
2. 多进程




### docker - 一站式N个编程语言开发环境
1. 阿里开源镜像：https://help.aliyun.com/zh/alinux/user-guide/pytorch-image-release-notes
2. awesom-compose：https://github.com/docker/awesome-compose
3. paddle学习案例：https://www.paddlepaddle.org.cn/documentation/docs/zh/install/compile/linux-compile-by-make.html#compile_from_docker
4. c++ 调式：https://github.com/microsoft/vscode-docs/blob/main/docs/cpp/cpp-debug.md
5. wsl docker cuda：https://duanyll.com/2024/6/30/WSL2-Docker-Deep-Learning/

### docker - vscode 环境配置
1. paddle镜像已内置
   1. 安装cpp插件
   2. 安装高版本go和goplfs
      1. Installing golang.org/x/tools/gopls@latest (/paddle/softwware/go/bin/gopls) SUCCEEDED
2. rust 
   1. curl --proto '=https' --tlsv1.2 https://sh.rustup.rs -sSf | sh
   2. rustup update 更新
   3. rustup self uninstall 卸载
   4. 安装插件 rust-analyzer 和java一样，打开一个项目就会自动索引，不用配置任何东西
```

设置指定路径。用于智能提示

{
    "C_Cpp.default.includePath": [
        "/paddle/Paddle/paddle"
    ],
    "go.gopath": "/paddle/softwware/go",
    "go.goroot": "/paddle/softwware/go",
}

```

### docker - cpp
1. 基于pytoch 或者 paddle
2. https://github.com/PaddlePaddle/Paddle/blob/develop/tools/dockerfile/Dockerfile.release.ubuntu20
3. https://hub.docker.com/layers/paddlepaddle/paddle/3.0.0b2-gpu-cuda12.3-cudnn9.0-trt8.6/images/sha256-996c18b0958a4e655ae5e9d3d8f35c91971bba7ab55024c85ff60d65242ecefa?context=explore


```
docker pull paddlepaddle/paddle:3.0.0b2-gpu-cuda12.3-cudnn9.0-trt8.6

docker pull paddlepaddle/paddle

docker pull registry.baidubce.com/paddlepaddle/paddle:3.0.0b1-jupyter



FROM ubuntu:18.04
MAINTAINER PaddlePaddle Authors <paddle-dev@baidu.com>

WORKDIR /workspace

ENV PATH /opt/python3/bin:/root/.local/bin:$PATH
ENV LD_LIBRARY_PATH $LD_LIBRARY_PATH:/opt/python3/lib

# Install Python
ADD https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/Miniconda3-4.7.12.1-Linux-x86_64.sh miniconda3.sh
RUN /bin/bash miniconda3.sh -b -p /opt/python3/ && \
    rm -f miniconda3.sh

RUN mkdir -p ~/.pip && \
    echo "[global]" >> ~/.pip/pip.conf && \
    echo "trusted-host =  mirrors.aliyun.com" >> ~/.pip/pip.conf && \
    echo "index-url = https://mirrors.aliyun.com/pypi/simple" >> ~/.pip/pip.conf

RUN echo "channels:" >> ~/.condarc && \
    echo "  - conda-forge" >> ~/.condarc && \
    echo "  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/" >> ~/.condarc && \
    echo "  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/" >> ~/.condarc && \
    echo "  - defaults" >> ~/.condarc && \
    echo "custom_channels:" >> ~/.condarc && \
    echo "  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud" >> ~/.condarc && \
    echo "  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud" >> ~/.condarc && \
    echo "  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud" >> ~/.condarc && \
    echo "  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud" >> ~/.condarc && \
    echo "  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud" >> ~/.condarc && \
    echo "  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud" >> ~/.condarc && \
    echo "show_channel_urls: true" >> ~/.condarc && \
    echo "channel_priority: strict" >> ~/.condarc

# Install R
RUN conda install -y r -c conda-forge

# Install PaddlePaddle
RUN /opt/python3/bin/python -m pip install \
    https://paddle-wheel.bj.bcebos.com/0.0.0-cpu-mkl/paddlepaddle-0.0.0-cp37-cp37m-linux_x86_64.whl

# Install reticulate, R interface to Python
RUN Rscript -e 'install.packages("reticulate", repos="https://cran.rstudio.com")'

COPY example example
RUN cd example && \
    curl -O https://paddle-inference-dist.cdn.bcebos.com/mobilenet-test-model-data.tar.gz && \
    tar -zxvf mobilenet-test-model-data.tar.gz && rm mobilenet-test-model-data.tar.gz

```

### paddle-docker-compose.yml
```
version: "3"
services:
  one-develop:
    build: .
    image: registry.baidubce.com/paddlepaddle/paddle:latest-dev
    container_name: cpp-python-go-rust
    # 使用vscode的remote-container无需设置ports,它将自动代理端口,可直接访问。
    # ports:
    #   - 3000:3000
    working_dir: /paddle
    network_mode: host  # 使用host网络模式
    volumes:
      - ".:/paddle"
    stdin_open: true   # 相当于-d 允许后台运行
    tty: true  # 相当于-i  允许交互

```

### python虚拟环境
```

python3.10 -m venv paddle-ml-env
source paddle-ml-env/bin/activate
python3.10 -m pip install requests

```

### docker - go
1. https://github.com/docker-library/golang/blob/4c0463340f0b14c2682af9d8d3bb8457a79f695d/1.23/alpine3.19/Dockerfile
```

```

### docker - rust
1. https://github.com/rust-lang/docker-rust/blob/1700955b94ae8a589562d872da74353028fffcf3/stable/alpine3.19/Dockerfile
2. python转rust：https://developer.aliyun.com/article/1218710
```
FROM alpine:3.19

LABEL org.opencontainers.image.source=https://github.com/rust-lang/docker-rust

RUN apk add --no-cache \
        ca-certificates \
        gcc

ENV RUSTUP_HOME=/usr/local/rustup \
    CARGO_HOME=/usr/local/cargo \
    PATH=/usr/local/cargo/bin:$PATH \
    RUST_VERSION=1.82.0

RUN set -eux; \
    apkArch="$(apk --print-arch)"; \
    case "$apkArch" in \
        x86_64) rustArch='x86_64-unknown-linux-musl'; rustupSha256='1455d1df3825c5f24ba06d9dd1c7052908272a2cae9aa749ea49d67acbe22b47' ;; \
        aarch64) rustArch='aarch64-unknown-linux-musl'; rustupSha256='7087ada906cd27a00c8e0323401a46804a03a742bd07811da6dead016617cc64' ;; \
        *) echo >&2 "unsupported architecture: $apkArch"; exit 1 ;; \
    esac; \
    url="https://static.rust-lang.org/rustup/archive/1.27.1/${rustArch}/rustup-init"; \
    wget "$url"; \
    echo "${rustupSha256} *rustup-init" | sha256sum -c -; \
    chmod +x rustup-init; \
    ./rustup-init -y --no-modify-path --profile minimal --default-toolchain $RUST_VERSION --default-host ${rustArch}; \
    rm rustup-init; \
    chmod -R a+w $RUSTUP_HOME $CARGO_HOME; \
    rustup --version; \
    cargo --version; \
    rustc --version;

```

#### rust库
```
Python库	Rust替代方案	教程
numpy	ndarray	Rust机器学习之ndarray
pandas	Polars	Rust机器学习之Polars
scikit-learn	Linfa	Rust机器学习之Linfa
matplotlib	plotters	Rust机器学习之plotters
pytorch	tch-rs	Rust机器学习之tch-rs
networks	petgraph	Rust机器学习之petgraph

```

#### Rust交互式编程环境搭建


## 从0到1安装
```bash
ubuntu
apt-get update
apt install sudo -y
# programm
apt install -y gcc-9 g++-9

alias gcc=/usr/bin/gcc-9
alias g++=/usr/bin/g++-9

# golang 1.20版本
wget https://mirrors.ustc.edu.cn/golang/go1.21.0.linux-amd64.tar.gz
tar -zxvf go1.21.0.linux-amd64.tar.gz
rm -rf /usr/local/go && tar -C /usr/local -xzf go1.21.0.linux-amd64.tar.gz
rm -rf /usr/bin/go && tar -C /usr/bin -xzf go1.21.0.linux-amd64.tar.gz
export PATH=$PATH:/usr/local/go/bin
go version
go env -w GO111MODULE=on
go env -w GOPROXY=https://goproxy.cn,direct

# pg库
apt-get install libpq-dev -y
apt install postgresql -y

# 创建库表 - 业务操作
# service postgresql start
# sudo -u postgres psql
# CREATE USER dbusername WITH PASSWORD 'dbpassword';
# CREATE DATABASE db;


# python3
apt install -y python3.8
apt install -y python3-pip
alias pip=pip3
alias python=python3
# openGL库
apt-get install -y libgl1-mesa-glx
# 进度条库
pip3 install tqdm   
# sql
pip3 install SQLAlchemy==1.4.23 -i https://mirrors.aliyun.com/pypi/simple
pip3 install psycopg2 


# cuda安装
wget https://developer.download.nvidia.com/compute/cuda/12.4.1/local_installers/cuda_12.4.1_550.54.15_linux.run
sudo sh cuda_12.4.1_550.54.15_linux.run


# pytorch
pip3 install torch torchvision -i https://mirrors.aliyun.com/pypi/simple

pip3 install graphviz -i https://mirrors.aliyun.com/pypi/simple

# huggingface
pip3 install setuptools_rust
pip3 install transformers

# juypter
pip3 install setuptools_scm
pip3 install argon2-cffi-bindings
pip3 install jupyter



# paddlepaddle 全家桶
pip3 install paddlepaddle==2.5.1 -i https://mirrors.aliyun.com/pypi/simple
pip3 install paddleocr
pip3 install visualdl==2.4.0  
pip3 install flask-babel==1.0.0
pip3 install paddlenlp -i https://mirrors.aliyun.com/pypi/simple

# pdf工具
pip3 install pdf2image
pip3 install gradio
pip3 install pdfplumber   

pip3 install langchain
# embedding
pip3 install fastNLP  
pip3 install fuzzywuzzy
python3 -m pip install milvus
pip3 install jieba

# modelscope
pip3 install numpy pandas urllib3
pip3 install datasets==2.13.0
pip3 install dill==0.3.6
pip3 install multiprocess==0.70.14
pip3 install accelerate -U
pip3 install sentencepiece -U
pip3 install modelscope -U 

# 打包当前环境
pip3 freeze > requirements.txt



# 业务逻辑
# llm_demo服务
dir=`pwd`
export PYTHONPATH=$PYTHONPATH:$dir
export PYTHONPATH=$PYTHONPATH:/mnt/workspace

# 写入数据
cd /mnt/workspace/llm_demo/util
python3 insert_balance_sheet.py
python3 insert_cash_flow.py
python3 insert_company_annual_reports.py
python3 insert_profit_statement.py

mv /etc/dsw/lib/libffi.so.8.1.0 /mnt/workspace/libffi/.
mv /etc/dsw/lib/libffi.so.8 /mnt/workspace/libffi/.
mv /opt/conda/lib/libffi.so.8 /mnt/workspace/libffi/.
mv /opt/conda/lib/libffi.so.8.1.0 /mnt/workspace/libffi/.
mv /opt/conda/pkgs/libffi-3.4.2-h6a678d5_6/lib/libffi.so.8.1.0 /mnt/workspace/libffi/.
mv /opt/conda/pkgs/libffi-3.4.2-h6a678d5_6/lib/libffi.so.8 /mnt/workspace/libffi/.


cp /mnt/workspace/libffi/libffi.so.8 /opt/conda/lib/libffi.so.8 



```

## 包编译
### 全局配置
cur_dir=/docker/root/projects/demo/package
export PATH="$cur_dir/cmake-3.24.0-linux-x86_64/bin:$PATH"
export PATH="$cur_dir/lib:$cur_dir/bin:$PATH"

### 拼表3.16.0
export PATH=$cur_dir/protobuf/build_source/installed_protobuf_lib/bin:${PATH}



### cmake
wget https://cmake.org/files/v3.24/cmake-3.24.0-linux-x86_64.tar.gz
tar xf cmake-3.24.0-linux-x86_64.tar.gz
export PATH=$cur_dir/include:$cur_dir/bin:$cur_dir/cmake-3.24.0-linux-x86_64/bin:/root/.nvm/versions/node/v14.17.2/bin:/root/.nvm/versions/node/v14.17.2/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
export LD_LIBRARY_PATH=/usr/local/lib
export LIBRARY_PATH=/usr/local/lib:$cur_dir/lib:$cur_dir/include
export LAPACK_LIBRARIES=/usr/local/lib


### python3.9.7
```shell
cur_dir=/docker/root/projects/demo/package

cd $cur_dir
https://www.python.org/ftp/python/3.9.7/Python-3.9.7.tar.xz
tar -xf Python-3.9.7.tar.xz

cd Python-3.9.7
./configure --prefix=$cur_dir/python3.9
make
make install

export PATH="$cur_dir/python3.9/bin:$PATH"


```

### 基础库
```
# gfortran
apt-get install gfortran -y

# libsndfile
apt-get install libsndfile1 -y

# OpenBLAS
apt-get install build-essential -y
git clone https://github.com/xianyi/OpenBLAS.git
cd OpenBLAS
# x86_64
make TARGET=NEHALEM
make
make PREFIX=$cur_dir install


# LAPACK
# 参考 https://blog.csdn.net/dante0610/article/details/113853805
apt-get install libscalapack-mpi-dev -y
apt-get install liblapack-dev -y

wget http://www.netlib.org/lapack/lapack-3.4.2.tgz
tar -xvf lapack-3.4.2.tgz
cd lapack-3.4.2
mkdir build
cd build
cmake ..
make -j4
make PREFIX=$cur_dir install

# gTest
git clone --depth 1 https://github.com/google/googletest.git
cd googletest
mkdir build && cd build
cmake ..
make -j40
make PREFIX=$cur_dir install

```

### faiss
/docker/root/projects/demo/package/lib/libopenblas.a

cmake -B build -DLAPACK_LIBRARIES=/usr/local/lib/liblapack.a -DBLAS_LIBRARIES=$cur_dir/lib/libopenblas.a  -DFAISS_ENABLE_GPU=OFF .
make -C build clean && make -C build -j40 && make -C build PREFIX=$cur_dir install

cmake -B build -DLAPACK_LIBRARIES=/usr/lib/x86_64-linux-gnu/lapack -DBLAS_LIBRARIES=$cur_dir/lib  -DFAISS_ENABLE_GPU=OFF -DBUILD_SHARED_LIBS=ON .


cd build
make -j40
make PREFIX=$cur_dir install

### puck


### starrocks


### pytorch源码安装
```bash


# g++安装
apt-get update
apt install sudo -y
# programm
apt install -y gcc-9 g++-9
apt install openssl libssl-dev
apt install -y build-essential
apt-get install -y patchelf
# python安装
apt install -y python3.8
apt install -y python3-pip
apt install -y python3.9-dev
pip install --upgrade pip
pip install --upgrade setuptools


# cmake安装
# 略

# 安装conda
wget https://repo.anaconda.com/archive/Anaconda3-2023.09-0-Linux-x86_64.sh

# 自动化安装
sh Anaconda3-2023.09-0-Linux-x86_64.sh -b -u
# 国内镜像源
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
conda config --set show_channel_urls yes

conda install -c https://mirrors.aliyun.com/anaconda/ intel::mkl-static intel::mkl-include -y
# CUDA only: Add LAPACK support for the GPU if needed
conda install -c pytorch magma-cuda110  # or the magma-cuda* that matches your CUDA version from https://anaconda.org/pytorch/repo


alias pip=pip3
alias python=python3
alias gcc=/usr/bin/gcc
alias g++=/usr/bin/g++
# gcc 路径会在pytorch中被用到
mv /usr/bin/gcc-9 /usr/bin/gcc
mv /usr/bin/g++-9 /usr/bin/g++
export PATH="/root/anaconda3/bin:$PATH"

# cmake 配置 指定编译器
export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-"$(dirname $(which conda))/../"}
export CMAKE_C_COMPILER=/usr/bin/gcc
export CMAKE_CXX_COMPILER=/usr/bin/g++




# Ninja 生成代码需要指定路径 手动修改
# export CMAKE_BUILD_WITH_INSTALL_RPATH=ON
# set(CMAKE_BUILD_WITH_INSTALL_RPATH TRUE)


# 安装第三方依赖，只能用pip
# conda install --file requirements.txt
conda install -c https://mirrors.aliyun.com/anaconda/ cmake ninja -y
conda install cmake ninja -y
pip install -i https://mirrors.aliyun.com/pypi/simple pyqt5==5.15
pip install -i https://mirrors.aliyun.com/pypi/simple pyqtwebengine==5.15
pip install -i https://mirrors.aliyun.com/pypi/simple -r requirements.txt



# 代码依赖下载
git clone --recursive https://github.com/pytorch/pytorch
cd pytorch
git submodule sync
git submodule update --init --recursive

# 国内访问限制问题，需要手动clone
cd third_party

## https://gitclone.com/github.com/xxx 可以加速 git submodule
# 参考
# https://github.com/NVlabs/cub.git 改成下面
# https://gitclone.com/github.com/NVlabs/cub.git
rm -rf ios-cmake
git clone https://521github.com/Yangqing/ios-cmake.git

rm -rf psimd
git clone https://521github.com/Maratyszcza/psimd.git


rm -rf QNNPACK
git clone https://521github.com/pytorch/QNNPACK

rm -rf foxi
git clone https://521github.com/houseroad/foxi.git
cd ..




# (optional) If using torch.compile with inductor/triton, install the matching version of triton
# Run from the pytorch directory after cloning
make triton

export MAX_JOBS=4
python setup.py develop > build.log



```

### python直接安装
```
pip install torchvision


```

### pytorch配置
```
https://mirror.tuna.tsinghua.edu.cn/help/anaconda/

先执行 conda config --set show_channel_urls yes 生成该文件之后再修改

channels:
  - defaults
show_channel_urls: true
default_channels:
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2
custom_channels:
  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  deepmodeling: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/


```

### pytorch submodule修改
```
[submodule "third_party/pybind11"]
    ignore = dirty
    path = third_party/pybind11
    url = https://gitclone.com/github.com/pybind/pybind11.git
[submodule "third_party/cub"]
    ignore = dirty
    path = third_party/cub
    url = https://gitclone.com/github.com/NVlabs/cub.git
[submodule "third_party/eigen"]
    ignore = dirty
    path = third_party/eigen
    url = https://gitlab.com/libeigen/eigen.git
[submodule "third_party/googletest"]
    ignore = dirty
    path = third_party/googletest
    url = https://gitclone.com/github.com/google/googletest.git
[submodule "third_party/benchmark"]
    ignore = dirty
    path = third_party/benchmark
    url = https://gitclone.com/github.com/google/benchmark.git
[submodule "third_party/protobuf"]
    ignore = dirty
    path = third_party/protobuf
    url = https://gitclone.com/github.com/protocolbuffers/protobuf.git
[submodule "third_party/ios-cmake"]
    ignore = dirty
    path = third_party/ios-cmake
    url = https://gitclone.com/github.com/Yangqing/ios-cmake.git
[submodule "third_party/NNPACK"]
    ignore = dirty
    path = third_party/NNPACK
    url = https://gitclone.com/github.com/Maratyszcza/NNPACK.git
[submodule "third_party/gloo"]
    ignore = dirty
    path = third_party/gloo
    url = https://gitclone.com/github.com/facebookincubator/gloo
[submodule "third_party/NNPACK_deps/pthreadpool"]
    ignore = dirty
    path = third_party/pthreadpool
    url = https://gitclone.com/github.com/Maratyszcza/pthreadpool.git
[submodule "third_party/NNPACK_deps/FXdiv"]
    ignore = dirty
    path = third_party/FXdiv
    url = https://gitclone.com/github.com/Maratyszcza/FXdiv.git
[submodule "third_party/NNPACK_deps/FP16"]
    ignore = dirty
    path = third_party/FP16
    url = https://gitclone.com/github.com/Maratyszcza/FP16.git
[submodule "third_party/NNPACK_deps/psimd"]
    ignore = dirty
    path = third_party/psimd
    url = https://gitclone.com/github.com/Maratyszcza/psimd.git
[submodule "third_party/zstd"]
    ignore = dirty
    path = third_party/zstd
    url = https://gitclone.com/github.com/facebook/zstd.git
[submodule "third_party/cpuinfo"]
    ignore = dirty
    path = third_party/cpuinfo
    url = https://gitclone.com/github.com/pytorch/cpuinfo.git
[submodule "third_party/python-peachpy"]
    ignore = dirty
    path = third_party/python-peachpy
    url = https://gitclone.com/github.com/malfet/PeachPy.git
[submodule "third_party/onnx"]
    ignore = dirty
    path = third_party/onnx
    url = https://gitclone.com/github.com/onnx/onnx.git
[submodule "third_party/onnx-tensorrt"]
    ignore = dirty
    path = third_party/onnx-tensorrt
    url = https://gitclone.com/github.com/onnx/onnx-tensorrt
[submodule "third_party/sleef"]
    ignore = dirty
    path = third_party/sleef
    url = https://gitclone.com/github.com/shibatch/sleef
[submodule "third_party/ideep"]
    ignore = dirty
    path = third_party/ideep
    url = https://gitclone.com/github.com/intel/ideep
[submodule "third_party/nccl/nccl"]
    ignore = dirty
    path = third_party/nccl/nccl
    url = https://gitclone.com/github.com/NVIDIA/nccl
[submodule "third_party/gemmlowp/gemmlowp"]
    ignore = dirty
    path = third_party/gemmlowp/gemmlowp
    url = https://gitclone.com/github.com/google/gemmlowp.git
[submodule "third_party/QNNPACK"]
    ignore = dirty
    path = third_party/QNNPACK
    url = https://gitclone.com/github.com/pytorch/QNNPACK
[submodule "third_party/neon2sse"]
    ignore = dirty
    path = third_party/neon2sse
    url = https://gitclone.com/github.com/intel/ARM_NEON_2_x86_SSE.git
[submodule "third_party/fbgemm"]
    ignore = dirty
    path = third_party/fbgemm
    url = https://gitclone.com/github.com/pytorch/fbgemm
[submodule "third_party/foxi"]
    ignore = dirty
    path = third_party/foxi
    url = https://gitclone.com/github.com/houseroad/foxi.git
[submodule "third_party/tbb"]
    path = third_party/tbb
    url = https://gitclone.com/github.com/01org/tbb
    branch = tbb_2018
[submodule "android/libs/fbjni"]
    ignore = dirty
    path = android/libs/fbjni
    url = https://gitclone.com/github.com/facebookincubator/fbjni.git
[submodule "third_party/XNNPACK"]
    ignore = dirty
    path = third_party/XNNPACK
    url = https://gitclone.com/github.com/google/XNNPACK.git
[submodule "third_party/fmt"]
    ignore = dirty
    path = third_party/fmt
    url = https://gitclone.com/github.com/fmtlib/fmt.git
[submodule "third_party/tensorpipe"]
    ignore = dirty
    path = third_party/tensorpipe
    url = https://gitclone.com/github.com/pytorch/tensorpipe.git
[submodule "third_party/cudnn_frontend"]
	path = third_party/cudnn_frontend
	url = https://gitclone.com/github.com/NVIDIA/cudnn-frontend.git
[submodule "third_party/kineto"]
    path = third_party/kineto
    url = https://gitclone.com/github.com/pytorch/kineto
[submodule "third_party/pocketfft"]
	path = third_party/pocketfft
	url = https://gitclone.com/github.com/mreineck/pocketfft
[submodule "third_party/ittapi"]
	path = third_party/ittapi
	url = https://gitclone.com/github.com/intel/ittapi.git
[submodule "third_party/flatbuffers"]
	path = third_party/flatbuffers
	url = https://gitclone.com/github.com/google/flatbuffers.git
[submodule "third_party/nlohmann"]
	path = third_party/nlohmann
	url = https://gitclone.com/github.com/nlohmann/json.git
[submodule "third_party/VulkanMemoryAllocator"]
	path = third_party/VulkanMemoryAllocator
	url = https://gitclone.com/github.com/GPUOpen-LibrariesAndSDKs/VulkanMemoryAllocator.git
[submodule "third_party/cutlass"]
	path = third_party/cutlass
	url = https://gitclone.com/github.com/NVIDIA/cutlass.git
[submodule "third_party/mimalloc"]
	path = third_party/mimalloc
	url = https://gitclone.com/github.com/microsoft/mimalloc.git


```
### pytorch-tune微调工具
```
PyTorch官方发布LLM微调工具TorchTune - 小小将的文章 - 知乎
https://zhuanlan.zhihu.com/p/688671130
```

### paddle源码安装
```shell
全局环境
apt-get update
apt install sudo -y
# programm
apt install -y gcc-9 g++-9

# 编译工具
apt install -y build-essential
apt-get install -y libffi-dev
apt-get install -y patchelf
apt install clang clangd lldb ccache -y

# python包
apt install -y python3.9
apt install -y python3.9-dev
apt install -y python3-pip
pip install --upgrade pip


cur_dir=/docker/root/projects/demo/package
export PATH="$cur_dir/cmake-3.24.0-linux-x86_64/bin:$PATH"
export PATH="$cur_dir/lib:$cur_dir/bin:$PATH"

git clone https://github.com/PaddlePaddle/Paddle.git
cd Paddle
git checkout 20ee0d7fbfb0034e37d86ceef7dd11059e5d3fad


pip install -t /usr/lib/python3.9/dist-packages/  -i https://mirrors.aliyun.com/pypi/simple  -r python/requirements.txt
pip install -i https://mirrors.aliyun.com/pypi/simple numpy
pip install -i https://mirrors.aliyun.com/pypi/simple protobuf
pip install -i https://mirrors.aliyun.com/pypi/simple pyyaml

mkdir build && cd build
cmake .. -DPY_VERSION=3.8 -DWITH_GPU=OFF -DWITH_TESTING=ON -DCMAKE_EXPORT_COMPILE_COMMANDS=1
# 后台编译
make -j10 > paddle_compile_v1.log &

# 加速编译过程
ccache cmake .. -DPY_VERSION=3.8 -DWITH_GPU=OFF -DWITH_TESTING=ON -DCMAKE_EXPORT_COMPILE_COMMANDS=1
ccache make -j10 > paddle.log & 

查看python lib
python3 -c "import distutils.sysconfig as sysconfig; print(sysconfig.get_config_var('LIBDIR'))"


查看python include
python3 -c "from distutils.sysconfig import get_python_inc; print(get_python_inc())" 


linux 镜像安装
https://www.paddlepaddle.org.cn/documentation/docs/zh/develop/install/compile/linux-compile-by-make.html
docker pull registry.baidubce.com/paddlepaddle/paddle:latest-dev
docker pull registry.baidubce.com/paddlepaddle/paddle:latest-dev-cuda12.0-cudnn8.9-trt8.6-gcc12.2

docker pull paddlepaddle/paddle:latest-dev
docker pull paddlepaddle/paddle:latest-dev-cuda12.0-cudnn8.9-trt8.6-gcc12.2

docker run --name paddle-test -v $PWD:/paddle --network=host -it registry.baidubce.com/paddlepaddle/paddle:latest-dev /bin/bash


```
### paddle虚拟环境
```
HOME=~
export PYENV_ROOT="$HOME/.pyenv"
export PATH="$PYENV_ROOT/bin:$PATH"
export PYTHON_BUILD_MIRROR_URL="https://npm.taobao.org/mirrors/python/"
alias python=python3
alias pip=pip3



安装和激活
git clone https://gitee.com/mirrors/pyenv.git ~/.pyenv
git clone https://github.com/pyenv/pyenv-virtualenv.git  ~/.pyenv/plugins/pyenv-virtualenv


pyenv install 3.9
pyenv virtualenv 3.9.19 env39
pyenv local env39

```
### paddle ci
```
https://www.paddlepaddle.org.cn/documentation/docs/zh/develop/dev_guides/git_guides/paddle_ci_manual_cn.html

```

### paddle编译问题
```
找不到PYTHON_LIBRARIES PYTHON_INCLUDE_DIRS
解决：apt install -y python3.9-dev


cmake .. -DPY_VERSION=3.9 -DPYTHON_LIBRARIES=/usr/lib/python3.9  -DPYTHON_INCLUDE_DIRS=/usr/include/python3.8 -DWITH_GPU=OFF -DWITH_TESTING=ON

-- Found PythonInterp: /usr/bin/python3.9 (found suitable version "3.9.5", minimum required is "3.9") 
CMake Error at /docker/root/projects/demo/package/cmake-3.24.0-linux-x86_64/share/cmake-3.24/Modules/FindPackageHandleStandardArgs.cmake:230 (message):
  Could NOT find PythonLibs (missing: PYTHON_LIBRARIES PYTHON_INCLUDE_DIRS)
  (Required is at least version "3.9")
Call Stack (most recent call first):
  /docker/root/projects/demo/package/cmake-3.24.0-linux-x86_64/share/cmake-3.24/Modules/FindPackageHandleStandardArgs.cmake:594 (_FPHSA_FAILURE_MESSAGE)
  /docker/root/projects/demo/package/cmake-3.24.0-linux-x86_64/share/cmake-3.24/Modules/FindPythonLibs.cmake:310 (FIND_PACKAGE_HANDLE_STANDARD_ARGS)
  cmake/external/python.cmake:21 (find_package)
  cmake/third_party.cmake:305 (include)
  CMakeLists.txt:604 (include)


-- Configuring incomplete, errors occurred!
See also "/docker/root/projects/demo/github/Paddle/build/CMakeFiles/CMakeOutput.log".
See also "/docker/root/projects/demo/github/Paddle/build/CMakeFiles/CMakeError.log".
make: *** No targets specified and no makefile found.  Stop.



找不到numpy，其实已经安装了，只是cmake文件用的python解释器有问题，不够兼容
修改 python_module.cmake
"${PYTHON_EXECUTABLE}" "-c" 
改成 python3 
 execute_process(
      COMMAND
        "python3" "-c"
        "import re, ${module}; print(re.compile('/__init__.py.*').sub('',${module}.__file__))"
      RESULT_VARIABLE _${module}_status
      OUTPUT_VARIABLE _${module}_location
      ERROR_QUIET OUTPUT_STRIP_TRAILING_WHITESPACE)


解决：
pip3 install --upgrade --force-reinstall -i https://mirrors.aliyun.com/pypi/simple  numpy

-t /usr/lib/python3.9/dist-packages

-- Could NOT find PY_numpy (missing: PY_NUMPY) 
CMake Error at cmake/python_module.cmake:29 (message):
  python module numpy is not found
Call Stack (most recent call first):
  cmake/external/python.cmake:73 (find_python_module)
  cmake/third_party.cmake:305 (include)
  CMakeLists.txt:604 (include)



同理修改FindNumPy.cmake
 "${PYTHON_EXECUTABLE}" ${PROJECT_BINARY_DIR}
 为
 "python3" ${PROJECT_BINARY_DIR}


CMake Error at /docker/root/projects/demo/package/cmake-3.24.0-linux-x86_64/share/cmake-3.24/Modules/FindPackageHandleStandardArgs.cmake:230 (message):
  Could NOT find NumPy (missing: PYTHON_NUMPY_INCLUDE_DIR)
Call Stack (most recent call first):
  /docker/root/projects/demo/package/cmake-3.24.0-linux-x86_64/share/cmake-3.24/Modules/FindPackageHandleStandardArgs.cmake:594 (_FPHSA_FAILURE_MESSAGE)
  cmake/FindNumPy.cmake:41 (find_package_handle_standard_args)
  cmake/external/python.cmake:77 (find_package)
  cmake/third_party.cmake:305 (include)
  CMakeLists.txt:604 (include)

```



## 模型训练

### xgboost

### catboost
1. 训练接口以及参数说明：https://catboost.ai/docs/en/concepts/python-reference_catboost_fit
2. roc曲线绘制：https://catboost.ai/docs/en/concepts/python-reference_utils_get_roc_curve

```

fit(X,
    y=None,
    cat_features=None,
    text_features=None,
    embedding_features=None,
    sample_weight=None,
    baseline=None,
    use_best_model=None,
    eval_set=None,
    verbose=None,
    logging_level=None,
    plot=False,
    plot_file=None,
    column_description=None,
    verbose_eval=None,
    metric_period=None,
    silent=None,
    early_stopping_rounds=None,
    save_snapshot=None,
    snapshot_file=None,
    snapshot_interval=None,
    init_model=None,
    log_cout=sys.stdout,
    log_cerr=sys.stderr)


predict(data,
        prediction_type=None,
        ntree_start=0,
        ntree_end=0,
        thread_count=-1,
        verbose=None,
        task_type="CPU")

```

### lightboost
1. 训练接口以及参数说明：https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html#lightgbm.LGBMClassifier.fit
2. 入门使用：https://lightgbm.readthedocs.io/en/latest/Python-Intro.html

```

:
X (numpy array, pandas DataFrame, H2O DataTable's Frame (deprecated), scipy.sparse, list of lists of int or float of shape = [n_samples, n_features]) – Input feature matrix.

y (numpy array, pandas DataFrame, pandas Series, list of int or float of shape = [n_samples]) – The target values (class labels in classification, real numbers in regression).

sample_weight (numpy array, pandas Series, list of int or float of shape = [n_samples] or None, optional (default=None)) – Weights of training data. Weights should be non-negative.

init_score (numpy array, pandas DataFrame, pandas Series, list of int or float of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task) or shape = [n_samples, n_classes] (for multi-class task) or None, optional (default=None)) – Init score of training data.

eval_set (list or None, optional (default=None)) – A list of (X, y) tuple pairs to use as validation sets.

eval_names (list of str, or None, optional (default=None)) – Names of eval_set.

eval_sample_weight (list of array (same types as sample_weight supports), or None, optional (default=None)) – Weights of eval data. Weights should be non-negative.

eval_init_score (list of array (same types as init_score supports), or None, optional (default=None)) – Init score of eval data.

eval_metric (str, callable, list or None, optional (default=None)) – If str, it should be a built-in evaluation metric to use. If callable, it should be a custom evaluation metric, see note below for more details. If list, it can be a list of built-in metrics, a list of custom evaluation metrics, or a mix of both. In either case, the metric from the model parameters will be evaluated and used as well. Default: ‘l2’ for LGBMRegressor, ‘logloss’ for LGBMClassifier, ‘ndcg’ for LGBMRanker.

feature_name (list of str, or 'auto', optional (default='auto')) – Feature names. If ‘auto’ and data is pandas DataFrame, data columns names are used.

categorical_feature (list of str or int, or 'auto', optional (default='auto')) – Categorical features. If list of int, interpreted as indices. If list of str, interpreted as feature names (need to specify feature_name as well). If ‘auto’ and data is pandas DataFrame, pandas unordered categorical columns are used. All values in categorical features will be cast to int32 and thus should be less than int32 max value (2147483647). Large values could be memory consuming. Consider using consecutive integers starting from zero. All negative values in categorical features will be treated as missing values. The output cannot be monotonically constrained with respect to a categorical feature. Floating point numbers in categorical features will be rounded towards 0.

callbacks (list of callable, or None, optional (default=None)) – List of callback functions that are applied at each iteration. See Callbacks in Python API for more information.

init_model (str, pathlib.Path, Booster, LGBMModel or None, optional (default=None)) – Filename of LightGBM model, Booster instance or LGBMModel instance used for continue training.
```


### lr

### tree + lr
1. 分类模型中的 GBDT+LR vs. LR vs. GBDT - 林滨的文章 - 知乎 https://zhuanlan.zhihu.com/p/358608707
2. Feature transformations with ensembles of trees：https://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html
3. Gradient Boost Part 1 (of 4): Regression Main Ideas：https://www.youtube.com/watch?v=3CC4N4z3GJc

### DLRM
1. https://github.com/facebookresearch/dlrm?tab=readme-ov-file

### 双塔
1. Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations
2. 代码实现：https://github.com/shenweichen/DeepCTR/tree/master/deepctr/models
3. 【论文解读】SENet网络 - ashergaga的文章 - 知乎 https://zhuanlan.zhihu.com/p/80123284
4. dssm：https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cikm2013_DSSM_fullversion.pdf
5. 向量召回 All You Need is 双塔 - 朱翔宇的文章 - 知乎 https://zhuanlan.zhihu.com/p/339116577
6. 百度莫比乌斯：https://research.baidu.com/Public/uploads/5d12eca098d40.pdf


### 模型融合
1. ensemble：https://www.kaggle.com/code/cody11null/quick-ensemble


### 多模型训练
```
lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))

ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))

KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)

model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,
                              learning_rate=0.05, n_estimators=720,
                              max_bin = 55, bagging_fraction = 0.8,
                              bagging_freq = 5, feature_fraction = 0.2319,
                              feature_fraction_seed=9, bagging_seed=9,
                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)


score = rmsle_cv(lasso)
print("\nLasso score: {:.4f} ({:.4f})\n".format(score.mean(), score.std()))

score = rmsle_cv(ENet)
print("ElasticNet score: {:.4f} ({:.4f})\n".format(score.mean(), score.std()))

score = rmsle_cv(model_xgb)
print("Xgboost score: {:.4f} ({:.4f})\n".format(score.mean(), score.std()))

```


### 多模型融合
```
class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):
    def __init__(self, models):
        self.models = models
        
    # we define clones of the original models to fit the data in
    def fit(self, X, y):
        self.models_ = [clone(x) for x in self.models]
        
        # Train cloned base models
        for model in self.models_:
            model.fit(X, y)

        return self
    
    #Now we do the predictions for cloned models and average them
    def predict(self, X):
        predictions = np.column_stack([
            model.predict(X) for model in self.models_
        ])
        return np.mean(predictions, axis=1)   

```

### stack
```
class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):
    def __init__(self, base_models, meta_model, n_folds=5):
        self.base_models = base_models
        self.meta_model = meta_model
        self.n_folds = n_folds
   
    # We again fit the data on clones of the original models
    def fit(self, X, y):
        self.base_models_ = [list() for x in self.base_models]
        self.meta_model_ = clone(self.meta_model)
        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)
        
        # Train cloned base models then create out-of-fold predictions
        # that are needed to train the cloned meta-model
        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))
        for i, model in enumerate(self.base_models):
            for train_index, holdout_index in kfold.split(X, y):
                instance = clone(model)
                self.base_models_[i].append(instance)
                instance.fit(X[train_index], y[train_index])
                y_pred = instance.predict(X[holdout_index])
                out_of_fold_predictions[holdout_index, i] = y_pred
                
        # Now train the cloned  meta-model using the out-of-fold predictions as new feature
        self.meta_model_.fit(out_of_fold_predictions, y)
        return self

 #Do the predictions of all base models on the test data and use the averaged predictions as 
    #meta-features for the final prediction which is done by the meta-model
    def predict(self, X):
        meta_features = np.column_stack([
            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)
            for base_models in self.base_models_ ])
        return self.meta_model_.predict(meta_features)

```

### ensembing
1. 多个模型结果，直接以不同权重 相加
```

RMSE on the entire Train data when averaging

def rmsle(y, y_pred):
    return np.sqrt(mean_squared_error(y, y_pred))

print('RMSLE score on train data:')
print(rmsle(y_train,stacked_train_pred*0.70 +
               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))
ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15

```



### torch - wide & deep
1. pytorch 实现wide&deep算法开发流程 - 沛然自若的文章 - 知乎 https://zhuanlan.zhihu.com/p/720137073


### paddle


## 数据分布

### seaborn - 目标值分布情况
```python
import matplotlib.pyplot as plt  # Matlab-style plotting


import seaborn as sns
color = sns.color_palette()
sns.set_style('darkgrid')

from scipy import stats
from scipy.stats import norm, skew #for some statistics

sns.distplot(train['SalePrice'] , fit=norm);

# Get the fitted parameters used by the function
(mu, sigma) = norm.fit(train['SalePrice'])
print( '\n mu = {:.2f} and sigma = {:.2f}\n'.format(mu, sigma))

#Now plot the distribution
plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)],
            loc='best')
plt.ylabel('Frequency')
plt.title('SalePrice distribution')

#Get also the QQ-plot
fig = plt.figure()
res = stats.probplot(train['SalePrice'], plot=plt)
plt.show()

```
### 如何处理 - 分布向右或者向左倾斜
```

对数变换：这是处理右偏数据最常用的方法之一
平方根变换：类似于对数变换，但效果稍弱
倒数变换：对于正值数据，可以考虑使用倒数变换
Box-Cox变换：这是一种更为通用的方法，它包括了对数变换、平方根变换等作为特殊情况
数据分箱：将连续的数据分成几个区间，每个区间内的数据用该区间的平均值或中位数代替，这也可以在一定程度上缓解偏斜问题

```

### 如何处理 - 特征缺失
```

all_data_na = (all_data.isnull().sum() / len(all_data)) * 100
all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]
missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})
missing_data.head(20)

缺失填充
all_data["PoolQC"] = all_data["PoolQC"].fillna("None")
all_data["MiscFeature"] = all_data["MiscFeature"].fillna("None")
all_data["Alley"] = all_data["Alley"].fillna("None")
#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood
all_data["LotFrontage"] = all_data.groupby("Neighborhood")["LotFrontage"].transform(
    lambda x: x.fillna(x.median()))

for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):
    all_data[col] = all_data[col].fillna(0)



```

### 如何处理 - 每个字段的相关性
```
#Correlation map to see how features are correlated with SalePrice
corrmat = train.corr()
plt.subplots(figsize=(12,9))
sns.heatmap(corrmat, vmax=0.9, square=True)
```


### 如何处理 - 数值特征 转 类别特征
```
#MSSubClass=The building class
all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)


#Changing OverallCond into a categorical variable
all_data['OverallCond'] = all_data['OverallCond'].astype(str)


#Year and month sold are transformed into categorical features.
all_data['YrSold'] = all_data['YrSold'].astype(str)
all_data['MoSold'] = all_data['MoSold'].astype(str)

```

### 如何处理 - 类别特征转换
```
from sklearn.preprocessing import LabelEncoder
cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', 
        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', 
        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',
        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', 
        'YrSold', 'MoSold')
# process columns, apply LabelEncoder to categorical features
for c in cols:
    lbl = LabelEncoder() 
    lbl.fit(list(all_data[c].values)) 
    all_data[c] = lbl.transform(list(all_data[c].values))

# shape        
print('Shape all_data: {}'.format(all_data.shape))

```

### 如何处理 - 数据分箱
1. https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.special.boxcox1p.html
```
skewness = skewness[abs(skewness) > 0.75]
print("There are {} skewed numerical features to Box Cox transform".format(skewness.shape[0]))

from scipy.special import boxcox1p
skewed_features = skewness.index
lam = 0.15
for feat in skewed_features:
    #all_data[feat] += 1
    all_data[feat] = boxcox1p(all_data[feat], lam)

```

## 特征处理

### 小数据工作流
1. pandas + sklearn + pytorch + csv
2. 结合gpt能很好实现一个demo，故这里不过多介绍怎么使用
3. 针对大数据大模型情况给出详细方案


### 大数据工作流
1. spark + pytorch/paddle
2. pyspark 特征处理文档：https://spark.apache.org/docs/latest/ml-features.html#normalizer
3. 中间文件格式：parquet 或者 orc，csv不合适

### kaggle 示例
1. nn特征前置处理：https://www.kaggle.com/code/shixw125/1st-place-nn-model-public-0-507-private-0-513



### 数据分布
1. XGBoost不要求数据是正态分布的
2. xgboost支持离散类别特征进行onehot编码


### 离散值
1. 离散与否和字段取值是否为数值不存在必然的联系。因为离散型字段的取值可能是数值，比如city_id 字段，虽然它的取值都是数值类型，但相互之间并没有大小关系；数值型字段的取值也可能是字符，比如 most_recent_sales_range 字段，它的取值虽然是字符类型，但可以明显感觉到相互之间存在大小关系。
2. 一是缺失值状况，二是字段大概的取值范围与分布。
3. 离散特征的关注点在于特征值的数量分布，而数值特征则需关注其取值范围和异常值、离群点等
4. pandas.series 的 describe 方法分析其取值范围和区间
5. 具体操作
   1. 对时间段进行处理，简单起见，提取月份、星期几（工作日与周末）以及时间段（上午、下午、晚上、凌晨）
   2. 对新生成的购买月份离散字段进行字典排序编码；

#### onehot处理
```python
from pyspark.sql import SparkSession
from pyspark.ml.feature import StringIndexer, OneHotEncoder

# 创建 SparkSession
spark = SparkSession.builder.appName("example").getOrCreate()

# 示例数据
data = [
    (1, "red"),
    (2, "blue"),
    (3, "green"),
    (4, "red"),
    (4, "wang"),
    (4, "magneto")
]

# 创建 DataFrame
df = spark.createDataFrame(data, ["id", "color"])

# 创建 StringIndexer
indexer = StringIndexer(inputCol="color", outputCol="color_index")

# 应用 StringIndexer
indexed_df = indexer.fit(df).transform(df)

# 创建 OneHotEncoder
encoder = OneHotEncoder(inputCol="color_index", outputCol="color_onehot")

# 应用 OneHotEncoder
encoded_df = encoder.fit(indexed_df).transform(indexed_df)

# 显示结果
encoded_df.show()

# 结果
+---+-------+-----------+-------------+                                         
| id|  color|color_index| color_onehot|
+---+-------+-----------+-------------+
|  1|    red|        0.0|(4,[0],[1.0])|
|  2|   blue|        1.0|(4,[1],[1.0])|
|  3|  green|        2.0|(4,[2],[1.0])|
|  4|    red|        0.0|(4,[0],[1.0])|
|  4|   wang|        4.0|    (4,[],[])|
|  4|magneto|        3.0|(4,[3],[1.0])|
+---+-------+-----------+-------------+

   id    color  color_index          color_onehot
0   1      red          0.0  (1.0, 0.0, 0.0, 0.0)
1   2     blue          1.0  (0.0, 1.0, 0.0, 0.0)
2   3    green          2.0  (0.0, 0.0, 1.0, 0.0)
3   4      red          0.0  (1.0, 0.0, 0.0, 0.0)
4   4     wang          4.0  (0.0, 0.0, 0.0, 0.0)
5   4  magneto          3.0  (0.0, 0.0, 0.0, 1.0)

```

#### 批量 - onehot处理
```python
from pyspark.sql import SparkSession
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler
from pyspark.ml import Pipeline

# 创建 SparkSession
spark = SparkSession.builder.appName("example").getOrCreate()

# 示例数据
data = [
    (1, "red", "small"),
    (2, "blue", "medium"),
    (3, "green", "large"),
    (4, "red", "medium"),
    (5, "blue", "small"),
    (6, "green", "large")
]

# 创建 DataFrame
df = spark.createDataFrame(data, ["id", "color", "size"])

# 定义需要处理的列
categorical_cols = ["color", "size"]

# 创建 StringIndexer 和 OneHotEncoder
indexers = [StringIndexer(inputCol=col, outputCol=f"{col}_index") for col in categorical_cols]
encoders = [OneHotEncoder(inputCol=f"{col}_index", outputCol=f"{col}_onehot") for col in categorical_cols]

# 创建 Pipeline
pipeline = Pipeline(stages=indexers + encoders)

# 应用 Pipeline
transformed_df = pipeline.fit(df).transform(df)

# 显示结果
transformed_df.show()

```

### 连续值
1. 归一化处理
   1. xgboost不需要归一化处理：https://github.com/dmlc/xgboost/issues/357
   2. xgboost使用之前是否需要对数据进行归一化处理 或者 ONEHOT处理? - 风影忍着的回答 - 知乎 https://www.zhihu.com/question/60630393/answer/2032846051
2. 

#### 归一化处理
```python
# import pandas as pd

# # 指定 Parquet 文件的路径
# file_path = '/home/clouddev/project/github/zhubo/feature-5/feature-all/part-00002-8f294f50-c068-47c9-8728-485ffca97a4a-c000.snappy.parquet'

# # 读取 Parquet 文件
# df = pd.read_parquet(file_path)

# print(df.head(10))
# # 显示 DataFrame
# # print(df)


from pyspark.sql import SparkSession
from pyspark.ml.linalg import Vectors
from pyspark.ml.feature import VectorAssembler
import pandas as pd

# 初始化 Spark
spark = SparkSession.builder.appName("LeftJoinExample") \
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.extraJavaOptions", "-XX:+UseG1GC -XX:MaxGCPauseMillis=200") \
    .getOrCreate()

dataset = spark.createDataFrame(
    [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0)],
    ["id", "hour", "mobile", "userFeatures", "clicked"])

assembler = VectorAssembler(
    inputCols=["hour", "mobile", "userFeatures"],
    outputCol="features")

output = assembler.transform(dataset)
print("Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'")
output.select("features", "clicked").show(truncate=False)


# 指定 Parquet 文件的路径
file_path = '/home/clouddev/project/github/zhubo/feature-5/feature-all/part-00002-8f294f50-c068-47c9-8728-485ffca97a4a-c000.snappy.parquet'

# 读取 Parquet 文件
df = spark.read.parquet(file_path)

# 显示 DataFrame 的前 10 行
df.select(['aid', 'scaledFeatures']).limit(10).show(truncate=False)


# # 将 DataFrame 转换为 Pandas DataFrame

# pandas_df = df.select(['aid', 'scaledFeatures']).limit(10).toPandas()

# # 
# pd.set_option('display.width', None)  # 自动调整列宽
# pd.set_option('display.max_colwidth', None)  # 显示完整的列内容

# # 显示 Pandas DataFrame
# print(pandas_df)

```

#### 批量 - 归一化处理
```python

from pyspark.sql import SparkSession
from pyspark.ml.feature import MinMaxScaler, VectorAssembler, StandardScaler
from pyspark.ml import Pipeline

# 初始化 Spark
spark = SparkSession.builder.appName("BatchNormalizationExample") \
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.extraJavaOptions", "-XX:+UseG1GC -XX:MaxGCPauseMillis=200") \
    .getOrCreate()

# 创建示例数据集
dataset = spark.createDataFrame([
    (0, 18, 1.0, 0.0),
    (1, 24, 0.8, 1.0),
    (2, 30, 0.6, 2.0)
], ["id", "hour", "mobile", "userFeatures"])

# 指定需要归一化的列
columns_to_scale = ["hour", "mobile", "userFeatures"]

# 创建 VectorAssembler 对象，将每个数值列转换为向量列
assemblers = [VectorAssembler(inputCols=[col], outputCol=f"{col}_vec") for col in columns_to_scale]


# 创建 MinMaxScaler 对象
scalers = [StandardScaler(inputCol=f"{col}_vec", outputCol=f"{col}_scaled") for col in columns_to_scale]

# 创建 Pipeline
pipeline = Pipeline(stages=assemblers + scalers)

# 应用 Pipeline
model = pipeline.fit(dataset)
scaled_data = model.transform(dataset)

# 显示归一化后的数据
scaled_data.show(truncate=False)


```

#### 离散化处理
```python

from pyspark.sql import SparkSession
from pyspark.ml.feature import QuantileDiscretizer

# 初始化 Spark
spark = SparkSession.builder.appName("FeatureBinningExample") \
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.extraJavaOptions", "-XX:+UseG1GC -XX:MaxGCPauseMillis=200") \
    .getOrCreate()

# 创建示例数据集
dataset = spark.createDataFrame([
    (0, 18, 1.0, 0.0),
    (1, 24, 0.8, 1.0),
    (2, 30, 0.6, 2.0)
], ["id", "hour", "mobile", "userFeatures"])

# 指定需要离散化的列
columns_to_bin = ["hour", "mobile", "userFeatures"]

# 定义分箱的数量
num_buckets = 4

# 创建 QuantileDiscretizer 对象
discretizers = [QuantileDiscretizer(numBuckets=num_buckets, inputCol=col, outputCol=f"{col}_binned") for col in columns_to_bin]

# 创建 Pipeline
from pyspark.ml import Pipeline
pipeline = Pipeline(stages=discretizers)

# 应用 Pipeline
model = pipeline.fit(dataset)
binned_data = model.transform(dataset)

# 显示离散化后的数据
binned_data.show(truncate=False)

```

#### 批量 - 离散化处理
```python

from pyspark.sql import SparkSession
from pyspark.ml.feature import QuantileDiscretizer
from pyspark.ml import Pipeline

def batch_discretize(df, columns, num_buckets_dict):
    """
    批量离散化多个列，每个列的分箱数可以自定义。

    :param df: 输入的 DataFrame
    :param columns: 需要离散化的列名列表
    :param num_buckets_dict: 每个列对应的分箱数字典，键为列名，值为分箱数
    :return: 离散化后的 DataFrame
    """
    # 检查输入参数
    if not set(columns).issubset(set(df.columns)):
        raise ValueError("Some columns specified do not exist in the DataFrame.")
    
    if not all(col in num_buckets_dict for col in columns):
        raise ValueError("Not all columns have a corresponding number of buckets specified.")
    
    # 创建 QuantileDiscretizer 对象
    discretizers = [
        QuantileDiscretizer(numBuckets=num_buckets_dict[col], inputCol=col, outputCol=f"{col}_binned")
        for col in columns
    ]
    
    # 创建 Pipeline
    pipeline = Pipeline(stages=discretizers)
    
    # 应用 Pipeline
    model = pipeline.fit(df)
    binned_data = model.transform(df)
    
    return binned_data

# 初始化 Spark
spark = SparkSession.builder.appName("BatchDiscretizationExample") \
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.extraJavaOptions", "-XX:+UseG1GC -XX:MaxGCPauseMillis=200") \
    .getOrCreate()

# 创建示例数据集
dataset = spark.createDataFrame([
    (0, 18, 1.0, 0.0),
    (1, 24, 0.8, 1.0),
    (2, 30, 0.6, 2.0)
], ["id", "hour", "mobile", "userFeatures"])

# 指定需要离散化的列及其分箱数
columns_to_bin = ["hour", "mobile", "userFeatures"]
num_buckets_dict = {
    "hour": 3,
    "mobile": 4,
    "userFeatures": 5
}

# 批量离散化
binned_data = batch_discretize(dataset, columns_to_bin, num_buckets_dict)

# 显示离散化后的数据
binned_data.show(truncate=False)
```


### 二阶特征


### 文本特征
1. 就是基于 CountVector和NLP 领域的 TF-IDF 向量特征



### 时序数据处理手段
1. Tutorial: Time Series Analysis and Forecasting：https://www.kaggle.com/code/satishgunjal/tutorial-time-series-analysis-and-forecasting



### 模型训练时特征权重调整


### 树模型输出的特征获取


### 训练集和测试集中特征分布差异
1. 差异过大的话，训练集训练的特征可能效果不是特别好，或者说泛化能力差


### 特征筛选 低频
1. 皮尔逊相关系数


### KFold 低频
1. https://www.kaggle.com/code/satishgunjal/tutorial-k-fold-cross-validation
2. 无法处理时序数据！！！

```

# Lets split the data into 5 folds.  
# We will use this 'kf'(KFold splitting stratergy) object as input to cross_val_score() method
kf =KFold(n_splits=5, shuffle=True, random_state=42)

cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1


交叉验证公式
#Validation function
n_folds = 5

def rmsle_cv(model):
    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)
    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring="neg_mean_squared_error", cv = kf))
    return(rmse)

```

## 样本处理
1. 纯量产经验：谈谈目标检测中正负样本的问题 - Mr.Jian的文章 - 知乎 https://zhuanlan.zhihu.com/p/563196861
2. 深度学习中，是否应该打破正负样本1:1的迷信思想？ - 密排六方橘子的回答 - 知乎 https://www.zhihu.com/question/654186093/answer/3483543427
3. 深度学习中，是否应该打破正负样本1:1的迷信思想？ - 刘芷宁的回答 - 知乎 https://www.zhihu.com/question/654186093/answer/3537147315
4. IMBENS:多类别不平衡数据处理与集成学习模型库 - 刘芷宁的文章 - 知乎 https://zhuanlan.zhihu.com/p/376572330
5. 有关类别不平衡数据(长尾)机器学习的一切：论文，代码，框架与库 - 刘芷宁的文章 - 知乎 https://zhuanlan.zhihu.com/p/111460698

### 概念
1. 正样本（positive）
2. 忽略样本（ignore）
3. 负样本（negative）
4. 正负样本 1：1 是过时且迷信的观点：https://www.zhihu.com/question/654186093
5. 损失函数的调整：
   1. 通过调整损失函数，可以使得模型更加关注少数类。例如，Focal Loss是一种常用的方法，它通过给少数类更高的权重，来减少模型对多数类的过度关注。
   2. 算法层面的改进：有些算法或模型结构专门设计用来处理不平衡数据。例如，代价敏感学习就是一种考虑了类别不平衡的算法，它通过为不同类别的错误分配不同的代价，来提高模型对少数类的识别能力。

### 负采样
```
# 对训练集进行负采样
df_train_neg = data[(data['is_attributed'] == 0)&(data['day'] < 9)]
df_train_neg = df_train_neg.sample(n=1000000)

# 合并成新的数据集
df_rest = data[(data['is_attributed'] == 1)|(data['day'] >= 9)]
data = pd.concat([df_train_neg, df_rest]).sample(frac=1)
del df_train_neg
del df_rest
gc.collect()

```