## 整体过程

```mermaid
graph LR
Kafka
ClickHouse
playback(数据转换)
search(回放平台)
Kafka-->playback
playback-->ClickHouse
ClickHouse-->search

```


## 服务

```mermaid
graph LR
Kafka
ClickHouse
KafkaService
ClickHouseService
playbackService

Kafka-->KafkaService
ClickHouseService-->ClickHouse

KafkaService-->playbackService
playbackService-->ClickHouseService


```

## 数据转换

```mermaid
graph LR
Kafka
ClickHouse
playback(数据转换)

Kafka-->playback
playback-->ClickHouse
```





## KafkaService
```mermaid
graph LR
Kafka
KafkaService
JsonService
ClickHouseService
MessageClass

Kafka-->|1.提供消息|KafkaService
KafkaService-->|2.消息格式转换|MessageClass
MessageClass-->|3.提供json格式|KafkaService
JsonService-->|4.提供json定位服务|KafkaService
JsonService-->|5.提供json对象转SQL服务|KafkaService


KafkaService-->|6.提供SQL语句|ClickHouseService



```


## ClickHouseService
```mermaid
graph LR
KafkaService
ClickHouseService

OperationType 
PlayBackTableClass


KafkaService-->|提供日志|ClickHouseService
ClickHouseService-->|日志分析得出用户行为|OperationType
ClickHouseService-->|日志分析得出有效时间点|operation_timestamp


OperationType-->1(输入搜索词)
OperationType-->2(点击sug)
OperationType-->3(修改筛选条件)
OperationType-->4(点击搜索结果)
OperationType-->5(进入结果详情)
OperationType-->6(拨打400电话)



OperationType-->|写入|PlayBackTableClass
operation_timestamp-->|写入|PlayBackTableClass
```

## 用户行为逻辑

- 问题：如何从已有的日志数据提取我们想要的有用的数据呢？



```mermaid
graph LR
log
search_log
click_log
search_words


log-->click_log
log-->search_log

search_log-->search_words

click_log-->sort
click_log-->filter
```



## 多线程PlayBackService

- 原则
  - run函数：必须只执行一个工作
  - 线程池数据必须加约束条件
- 方案
  - 只生成insert语句
  - 用队列维护数据，确保多线程下操作不冲突
- 问题
  - 高并发大流量情况下，连接池始终不够用
  - 而且数据库那边拒绝100个以上的并发请求
  - 不过小流量情况，跑的挺稳定的



```mermaid
graph TD
MessageQueue
InsertQueue
PlayBackService
ClickHouseService
ConnectionPool

Kafka-->MessageQueue
MessageQueue-->PlayBackService1
MessageQueue-->PlayBackService2
MessageQueue-->PlayBackService3



PlayBackService1-->InsertQueue
PlayBackService2-->InsertQueue
PlayBackService3-->InsertQueue

InsertQueue-->PlayBackService
PlayBackService-->Connection1
PlayBackService-->Connection2
PlayBackService-->Connection3
Connection1-->ConnectionPool
Connection2-->ConnectionPool
Connection3-->ConnectionPool


ConnectionPool-->ClickHouseService

```

## PlayBackTable的填充
```mermaid
graph TD
PlayBackTable-->time
PlayBackTable-->Summary
Summary-->Type1(点击)
Summary-->Type2(无点击)
Type2-->Type3(搜索)
Type2-->Type4(排序)
Type2-->Type5(过滤)
Type2-->Type6(sug)

```



## PlayBackTypeTable的填充

```mermaid
graph LR
PlayBackTable



PlayBackTable-->PlayBackTypeTable1
PlayBackTable-->PlayBackTypeTable2
PlayBackTable-->PlayBackTypeTable3
PlayBackTable-->PlayBackTypeTable4
PlayBackTable-->PlayBackTypeTable5
PlayBackTable-->PlayBackTypeTable6

问题
```





## 问题

- clickhouse连接池超时问题
  - 单连接方案
  - 自己建立连接池
  - 使用C3P0可是不熟悉，折腾到后面也没有跑动代码
  - 用clickhouseDataSource明显减少这个问题
  - 如何达到一万条不漏数据呢！
  - 因为连接超时丢出的是异常！所以在catch段中在执行一次。目前1万条数据，不少1条。可以说比较完美的解决方案了
  - 

## 回放平台



